{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fec9df",
   "metadata": {},
   "source": [
    "# Earth Observation Foundation Models and Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895fc04e",
   "metadata": {},
   "source": [
    "## Section A: Introduction\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### From Transfer Learning and Beyond\n",
    "\n",
    "---\n",
    "\n",
    "Transfer learning is a powerful technique in machine learning that allows a model trained on one task to be reused for a different but related task. Instead of starting from scratch, transfer learning leverages the knowledge a model has already gained from a large dataset and applies it to a new problem, which is particularly useful when labeled data for the new task is limited. Once trained, the lower layers of these models typically learn general features, such as edges in images or syntactic patterns in text, which can be useful across a range of tasks. By fine-tuning the upper layers of a pre-trained model or adding new layers specific to the new task, developers can adapt it to perform well with far fewer training examples. Properly executed transfer learning not only saves computational resources but also significantly reduces training time. It can also lead to better performance, particularly in scenarios where the target dataset is small or imbalanced. There are different strategies for transfer learning, such as freezing all or some of the layers during training, or allowing all layers to be updated depending on the similarity between the source and target tasks. For example, in medical imaging, models trained on natural images can be adapted to detect tumors or classify X-rays by fine-tuning only a few layers.\n",
    "\n",
    "<img src=\"./assets/pretraining_workflow.png\" style='height:400px'/>\n",
    "\n",
    "Figure 1. Simplified foundation model pretraining workflow\n",
    "\n",
    "An emerging category of models coined **'Foundation Models (FM)'** subset the transfer learning technique in which the model pretraining task is derived from raw unlabelled data rather than a specific task. In other words, these models are trained in a 'self-supervised' manner. These models potentially offer similar benefits to models developed using traditional transfer learning techniques (i.e reduced training time and performance). However, unlike with transfer learning, FMs circumvent the development of an intial large scale labelled task. This technique is especially powerful in the remote sensing world where the availability of large scale unprocessed data is unprecedented and the methods for transforming it into actionable insights draw from a wide range of scientific disciplines. **The larger goal of the earth observation foundation model (EOFM) development communitity is to create a model capable of generalizing across domains by learning from the structure and correlations iniherent in the raw data and open the door for more versatile models that can adapt with minimal fine-tuning. Attempts towards this ephemeral goal, however, must be thoroughly assessed in a way that is relavent to the greater remote sensing community.** We'll cover in this notebook: a simplified understanding of EOFMs, how to utilize them, benchmark their performance, and assess their utility for your downstream task.\n",
    "\n",
    "---\n",
    "\n",
    "###  Pretraining Methodologies\n",
    "\n",
    "---\n",
    "\n",
    "Below are 3 common methods for pretraining a computer vision model in a self-supervised fashion. This is by no means not an exhaustive list! Due to widespread availability of satellite data, a number of variations and/or combinations of the methods below exist along with a number of alternative techniques that are not in widespread use. There is also an extensive effort to marry the computer vision and language domains both in and out of the remote sensing community. For simplicity, we omit those from this notebook. It is worth nothing that the listed paradigms are all 'transformer-based' where the core architecural unit is a vision-transformer trained under the 'encoder-decoder' paradigm. While it is important to understand the high level distinction between these techniques, it is not entirely necessary in order to implement them in practice as many developer groups have worked hard in abstracting most of the logic away in various frameworks. Deep understanding of each of these model families as well as the nuances of a transformer is out of the scope of this notebook but we encourage folks to learn as much as possible!\n",
    "\n",
    "#### <u>Autoencoders</u> \n",
    "\n",
    "<img src=\"./assets/mae.png\" style='height:400px'/>\n",
    "\n",
    "Figure 2. Diagram depicting masked autoencoder training. Images are broken into patches that are subsequently embedded (typically with a linear layer) and masked. The decoder is tasked with reconstructing the original image from the unmasked context.\n",
    "\n",
    "Autoencoders consist of an encoder that compresses input images into a lower-dimensional latent space and a decoder that reconstructs the original images from these representations. This compression helps the model capture essential features while discarding noise or irrelevant details. The most dominent though not necessarily the most performant method is masked image modelling (i.e the 'Masked Autoencoder' (MAE)) where the model is tasked with reconstructing the masked part of an image from unmasked context. There are a handful of masking strageties that researchers have presented though they all work under the same overarching idea that a model optimized on the reconstruction task is generalizable, or able to perform well on unseen data.\n",
    "\n",
    "#### <u>Contrastive Learning</u> \n",
    "\n",
    "<img src=\"./assets/simclr.png\" style='height:400px'/>\n",
    "\n",
    "Figure 3. Diagram for SimCLR training paradigm.\n",
    "\n",
    "Contrastive models are tasked with distinguishing between similar and dissimilar image representations. It works by pulling together representations of positive pairs (different augmented views of the same image) while pushing apart representations of negative pairs, which are views from different images. Common methods include 'Simple Contrastive Learning' (SimCLR) and 'Momentum Contrast' (MoCo). Unlike autoencoders, a decoder is typically not included in the pretraining phase since the task is executed entirely in the latent space.\n",
    "\n",
    "#### <u>Non-contrastive Self Distillation</u> \n",
    "\n",
    "<img src=\"./assets/dino.png\" style='height:400px'/>\n",
    "\n",
    "Figure 4. Diagram for DINO training paradigm.\n",
    "\n",
    "Non-contrastive self-distillation is a self-supervised learning approach for computer vision that avoids the need for negative samples. Instead of contrasting different images, it trains a student network to match the output of a teacher network, both fed with different augmented views of the same image. The teacher is often an exponential moving average of the student, providing stable targets for the two encoders. The objective is to produce consistent, meaningful embeddings, not to reconstruct or generate the input. Common methods include 'Self-**di**stillation with **no** labels' (DINO) and 'Bootstrap Your Own Latent' (BYOL).\n",
    "\n",
    "---\n",
    "\n",
    "### Decoders\n",
    "\n",
    "---\n",
    "\n",
    "You're probably wondering at this point, what do you do with the trained encoder? Just as you would with any trained neural network, you can load the weights for inference using similar code you might have seen in previous chapters. There is however, one more consideration you must make depending on the type of task at hand and the shape of the output of the encoder. If, your goal is to classify images, a lightweight machine learning algorithm directly on top of the vector outputs of the encoders may be sufficient. The assumption here is that the internal representation of the image is robust and **relavent** to your task. These are the 'embeddings' you may have heard of recently and are analigous to large coarse resolution raster. We will touch on this topic in a separate chapter. If your goal, on the other hand, is to segment the images, we may need to train a subsequent neural network to extract from that internal representation and produce a useable map. Below is a summary list of contemporary decoders used in conjunction with transformer-based encoders. Each vary in complexity, purpose, and method in which they extract from the transformer layers in the aforementioned encoders.\n",
    "\n",
    "#### FCN\n",
    "\n",
    "<img src=\"./assets/fcn.png\" style='height:400px'/>\n",
    "\n",
    "Figure 5. Full convolutional neural network architecural diagram.\n",
    "\n",
    "The FCN is a stack of convolutional layers that upsamples and refines final feature maps produced by the transformerâ€™s output embeddings and converts them back into an image-like spatial representation. This final representation can then be fed through a final prediction layer, which is typically itself convolutional layers, to produce the final map. This is typically more lightweight than some of the MLP-based decoders below, due to weight sharing in the learned kernel filters. Local context is also explicitly emphasized as those same kernel filters may only cover a small portion of the whole input image.\n",
    "\n",
    "#### Segformer\n",
    "\n",
    "<img src=\"./assets/segformer.png\" style='height:400px'/>\n",
    "\n",
    "Figure 6. Segformer style network architecural diagram for semantic segmentation.\n",
    "\n",
    "Segformers use a MLP style decoder to extract from a hierarchical transformer-based encoder. Originally developed as a standalone framework for end to end training, the segformer has been recently adapted by a handful of research groups attempting to utilize the representation generated by the pretraining phase. The assumption is that the implementation of the transformer blocks in the encoder are sufficiently hierarchical in nature such that they can be composed using this method.\n",
    "\n",
    "#### UperNet\n",
    "\n",
    "<img src=\"./assets/upernet.png\" style='height:400px'/>\n",
    "\n",
    "Figure 7. UperNet architectural diagram.\n",
    "\n",
    "Similar to the segformer architecture, the upernet leverages multi-scale hierarchical features to generate dense predictions. Rather than MLPs, the upernet utilizes pooling layers and convolutions to compose the features before the final prediction layers. It's worth noting here that the upernet was designed with a ResNet-50 backbone that was fine tuned. In other words, this closely aligns with the 'pretrained encoder' with a decoder head methology that has been popularized in the last year.\n",
    "\n",
    "#### Linear Probe\n",
    "\n",
    "<img src=\"./assets/linear.png\" style='height:400px'/>\n",
    "\n",
    "Figure 8. Simplified structure of a single linear layer.\n",
    "\n",
    "The simplest decoder available. It is a single linear layer to process just the output of the encoder. In practice, this isn't typically used for segmentation due a limited model complexity. It is still useful, however, to assess representation quality as it is cheap to train and can be implemented in a consistently, making it an ideal tool for benchmarking. This technique is taken from the language domain where large language models are often evaluated on downstream tasks with linear probes on frozen embeddings before doing full fine-tuning. \n",
    "\n",
    "#### Muster\n",
    "\n",
    "<img src=\"./assets/muster.png\" style='height:400px'/>\n",
    "\n",
    "Figure 9. Architecural diagram of MUSTER decoder.\n",
    "\n",
    "A more recent decoder development in the computer vision realm. Similar to both the Segformer and the UperNet, hierarchical features play an important role in it's predictive strength with the most important distinction here being that the decoder itself is also transformer based. Also similar to the UperNet, the MUSTER decoder was designed to integrate with pretrained encoders. While this may lend it self to greater performance, it is important to consider the compute required to implement such an architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84a747",
   "metadata": {},
   "source": [
    "## Section B: The Framework\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "We build upon the Pangaea Bench benchmarking framework originally developed by the ESA Phi Lab team. While, there are other frameworks in development by other teams, Pangaea already has many of the more popular remote sensing foundation models integrated into their pure pytorch framework along with a number of well established benchmarking datasets. It also includes a relatively simple dataset integration scheme along with additional decoder heads integrated by the team at Spatial Informatics Group for the purpose of evaluating methods for extracting the information from the internal representation of the foundation model. The limited dependency is especially beneficial for small scale demonstrations such as this learning notebook. You will find similar themes found in previous chapters of the Applied Deep Learning Book such as dataloaders, pytorch modules, loss functions, etc. These are not exclusiive to training end to end models and in fact, its the almost exactly the same! The intended purpose of EOFMs is solving the same segmentation, classification, or regression problems. Here, we focus on the additional considerations that come with using an EOFM.\n",
    "\n",
    "<img src=\"./assets/pangaea_workflow.png\" style='height:400px'/>\n",
    "\n",
    "Figure 5. Diagram of Pangaea's general structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Software/Hardware considerations\n",
    "\n",
    "CUDA is the underlying software running NVidia GPUs. While it is not necessary, we recommend running this notebook in a CUDA enabled linux environment for convenience and setup simplicity. We also recommend installing python>=3.10 either standalone or through a conda managed environment. For running the model, we can estimate a VRAM required for storing the model using a simple calculation of # of parameters and precision. For example, a 300M parameter model at 32bit precision would roughly require 3.6GB to store the weights and optimizer states. Another 1.2 GB is required for the gradients of each of those parameters and a few more for the data itself. Likewise, the decoder used to extract from the internal representation also imposes VRAM / compute requirements. There's no universal formula here, but tools like PyTorch hooks, torch.cuda.memory_summary(), or profilers can help. 12GB of VRAM is a good place to start and luckily there are free collab options at this compute scale, but, there is a trend towards larger and more complex models which inevitably consume more compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51d3be",
   "metadata": {},
   "source": [
    "#### Installing framework and requirements\n",
    "\n",
    "We are installing directly from a cloned repo along with the available requirements.txt file. This may take a few moments as PyTorch is a fairly large package. Please install python and pip on your own and setup on your notebook environment that best suits your preferences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0400c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sig-gis/pangaea-bench.git \"pangaea-bench\"\n",
    "!pip install -e \"pangaea-bench\"\n",
    "!pip install -r pangaea-bench/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6ea02",
   "metadata": {},
   "source": [
    "#### The 'torchrun' Command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea32aab",
   "metadata": {},
   "source": [
    "#### Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5536b8",
   "metadata": {},
   "source": [
    "#### Logging and Model Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ab794",
   "metadata": {},
   "source": [
    "## Section C: Example Workflow: Burn Scar Mapping (MTBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02382640",
   "metadata": {},
   "source": [
    "### Frozen vs Unfrozen vs Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e329355",
   "metadata": {},
   "source": [
    "### Lets Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d80bb",
   "metadata": {},
   "source": [
    "#### DOFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_fcn\\\n",
    "    preprocessing=seg_default\\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84361e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_linear\\\n",
    "    preprocessing=seg_default\\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387064bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_muster\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceac53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_segformer\\\n",
    "    preprocessing=seg_resize_input_layer \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_upernet\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78614820",
   "metadata": {},
   "source": [
    "#### Prithvi 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d610e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HYDRA_FULL_ERROR=1 torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi \\\n",
    "    decoder=seg_fcn\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi \\\n",
    "    decoder=seg_linear\\\n",
    "    preprocessing=seg_resize_input_layer \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi \\\n",
    "    decoder=seg_muster\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b85067",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi\\\n",
    "    decoder=seg_segformer\\\n",
    "    preprocessing=seg_resize_input_layer \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c2b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi \\\n",
    "    decoder=seg_upernet\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177798b2",
   "metadata": {},
   "source": [
    "#### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a83af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_fcn\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_segformer\\\n",
    "    preprocessing=seg_resize_input_layer \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a792a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_linear\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_muster\\\n",
    "    preprocessing=seg_default\\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f662a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_upernet\\\n",
    "    preprocessing=seg_resize_input_layer \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.use_wandb=true \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b0e07",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3042c1",
   "metadata": {},
   "source": [
    "#### Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af6bf0b",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f44dd5",
   "metadata": {},
   "source": [
    "### Final Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7da3ff",
   "metadata": {},
   "source": [
    "#### A Well Defined Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620074fc",
   "metadata": {},
   "source": [
    "#### Multimodality / Multisensor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d23963",
   "metadata": {},
   "source": [
    "#### Scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eofm-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
