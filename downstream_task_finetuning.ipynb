{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fec9df",
   "metadata": {},
   "source": [
    "# Earth Observation Foundation Models and Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895fc04e",
   "metadata": {},
   "source": [
    "## Section A: Introduction\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### From Transfer Learning and Beyond\n",
    "\n",
    "---\n",
    "\n",
    "Transfer learning is a powerful technique in machine learning that allows a model trained on one task to be reused for a different but related task. Instead of starting from scratch, transfer learning leverages the knowledge a model has already gained from a large dataset and applies it to a new problem, which is particularly useful when labeled data for the new task is limited. Once trained, the lower layers of these models typically learn general features, such as edges in images or syntactic patterns in text, which can be useful across a range of tasks. By fine-tuning the upper layers of a pre-trained model or adding new layers specific to the new task, developers can adapt it to perform well with far fewer training examples. Properly executed transfer learning not only saves computational resources but also significantly reduces training time. It can also lead to better performance, particularly in scenarios where the target dataset is small or imbalanced. There are different strategies for transfer learning, such as freezing all or some of the layers during training, or allowing all layers to be updated depending on the similarity between the source and target tasks. For example, in medical imaging, models trained on natural images can be adapted to detect tumors or classify X-rays by fine-tuning only a few layers.\n",
    "\n",
    "<img src=\"./assets/pretraining_workflow.png\" style='height:400px'/>\n",
    "\n",
    "Figure 1. Simplified foundation model pretraining workflow\n",
    "\n",
    "An emerging category of models coined **'Foundation Models (FM)'** subset the transfer learning technique in which the model pretraining task is derived from raw unlabelled data rather than a specific task. In other words, these models are trained in a 'self-supervised' manner. These models potentially offer similar benefits to models developed using traditional transfer learning techniques (i.e reduced training time and performance). However, unlike with transfer learning, FMs circumvent the development of an intial large scale labelled task. This technique is especially powerful in the remote sensing world where the availability of large scale unprocessed data is unprecedented and the methods for transforming it into actionable insights draw from a wide range of scientific disciplines. **The larger goal of the earth observation foundation model (EOFM) development communitity is to create a model capable of generalizing across domains by learning from the structure and correlations iniherent in the raw data and open the door for more versatile models that can adapt with minimal fine-tuning. Attempts towards this ephemeral goal, however, must be thoroughly assessed in a way that is relavent to the greater remote sensing community.** \n",
    "\n",
    "We'll cover in this notebook: a simplified understanding of EOFMs, how to utilize them, benchmark their performance, and what to consider when assessing their utility for your downstream task. We assume that you have some familiarity with training deep learning model and the software associated with that process as well as a high level understanding of concepts such as network weights, back propagation, and checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "###  Pretraining Methodologies\n",
    "\n",
    "---\n",
    "\n",
    "Below are 3 common methods for pretraining a computer vision model in a self-supervised fashion. This is by no means not an exhaustive list! Due to widespread availability of satellite data, a number of variations and/or combinations of the methods below exist along with a number of alternative techniques that are not in widespread use. There is also an extensive effort to marry the computer vision and language domains both in and out of the remote sensing community. For simplicity, we omit those from this notebook. It is worth nothing that the listed paradigms are all 'transformer-based' where the core architecural unit is a vision-transformer trained under the 'encoder-decoder' paradigm. While it is important to understand the high level distinction between these techniques, it is not entirely necessary in order to implement them in practice as many developer groups have worked hard in abstracting most of the logic away in various frameworks. Deep understanding of each of these model families as well as the nuances of a transformer is out of the scope of this notebook but we encourage folks to learn as much as possible!\n",
    "\n",
    "#### <u>Autoencoders</u> \n",
    "\n",
    "<img src=\"./assets/mae.png\" style='height:400px'/>\n",
    "\n",
    "Figure 2. Diagram depicting masked autoencoder training. Images are broken into patches that are subsequently embedded (typically with a linear layer) and masked. The decoder is tasked with reconstructing the original image from the unmasked context.\n",
    "\n",
    "Autoencoders consist of an encoder that compresses input images into a lower-dimensional latent space and a decoder that reconstructs the original images from these representations. This compression helps the model capture essential features while discarding noise or irrelevant details. The most dominent though not necessarily the most performant method is masked image modeling (i.e the 'Masked Autoencoder' (MAE)) where the model is tasked with reconstructing the masked part of an image from unmasked context. There are a handful of masking strageties that researchers have presented though they all work under the same overarching idea that a model optimized on the reconstruction task is generalizable, or able to perform well on unseen data.\n",
    "\n",
    "#### <u>Contrastive Learning</u> \n",
    "\n",
    "<img src=\"./assets/simclr.png\" style='height:400px'/>\n",
    "\n",
    "Figure 3. Diagram for SimCLR training paradigm.\n",
    "\n",
    "Contrastive models are tasked with distinguishing between similar and dissimilar image representations. It works by pulling together representations of positive pairs (different augmented views of the same image) while pushing apart representations of negative pairs, which are views from different images. Common methods include 'Simple Contrastive Learning' (SimCLR) and 'Momentum Contrast' (MoCo). Unlike autoencoders, a decoder is typically not included in the pretraining phase since the task is executed entirely in the latent space.\n",
    "\n",
    "#### <u>Non-contrastive Self Distillation</u> \n",
    "\n",
    "<img src=\"./assets/dino.png\" style='height:400px'/>\n",
    "\n",
    "Figure 4. Diagram for DINO training paradigm.\n",
    "\n",
    "Non-contrastive self-distillation is a self-supervised learning approach for computer vision that avoids the need for negative samples. Instead of contrasting different images, it trains a student network to match the output of a teacher network, both fed with different augmented views of the same image. The teacher is often an exponential moving average of the student, providing stable targets for the two encoders. The objective is to produce consistent, meaningful embeddings, not to reconstruct or generate the input. Common methods include 'Self-**di**stillation with **no** labels' (DINO) and 'Bootstrap Your Own Latent' (BYOL).\n",
    "\n",
    "---\n",
    "\n",
    "### Decoders\n",
    "\n",
    "---\n",
    "\n",
    "You're probably wondering at this point, what do you do with the trained encoder? Just as you would with any trained neural network, you can load the weights for inference using similar code you might have seen in previous chapters. There is however, one more consideration you must make depending on the type of task at hand and the shape of the output of the encoder. If, your goal is to classify images, a lightweight machine learning algorithm directly on top of the vector outputs of the encoders may be sufficient. The assumption here is that the internal representation of the image is robust and **relavent** to your task. These are the 'embeddings' you may have heard of recently and are analigous to large coarse resolution raster. We will touch on this topic in a separate chapter. If your goal, on the other hand, is to segment the images, we may need to train a subsequent neural network to extract from that internal representation and produce a useable map. Below is a summary list of contemporary decoders used in conjunction with transformer-based encoders. Each vary in complexity, purpose, and method in which they extract features from the transformer layers in the aforementioned encoders.\n",
    "\n",
    "#### FCN\n",
    "\n",
    "<img src=\"./assets/fcn.png\" style='height:400px'/>\n",
    "\n",
    "Figure 5. Full convolutional neural network architecural diagram.\n",
    "\n",
    "The FCN is a stack of convolutional layers that upsamples and refines final feature maps produced by the transformer’s output embeddings and converts them back into an image-like spatial representation. This final representation can then be fed through a final prediction layer, which is typically itself convolutional layers, to produce the final map. This is typically more lightweight than some of the MLP-based decoders below, due to weight sharing in the learned kernel filters. Local context is also explicitly emphasized as those same kernel filters may only cover a small portion of the whole input image.\n",
    "\n",
    "#### Segformer\n",
    "\n",
    "<img src=\"./assets/segformer.png\" style='height:400px'/>\n",
    "\n",
    "Figure 6. Segformer style network architecural diagram for semantic segmentation.\n",
    "\n",
    "Segformers use a MLP style decoder to extract from a hierarchical transformer-based encoder. Originally developed as a standalone framework for end to end training, the segformer has been recently adapted by a handful of research groups attempting to utilize the representation generated by the pretraining phase. The assumption is that the implementation of the transformer blocks in the encoder are sufficiently hierarchical in nature such that they can be composed using this method.\n",
    "\n",
    "#### UperNet\n",
    "\n",
    "<img src=\"./assets/upernet.png\" style='height:400px'/>\n",
    "\n",
    "Figure 7. UperNet architectural diagram.\n",
    "\n",
    "Similar to the segformer architecture, the upernet leverages multi-scale hierarchical features to generate dense predictions. Rather than MLPs, the upernet utilizes pooling layers and convolutions to compose the features before the final prediction layers. It's worth noting here that the upernet was designed with a ResNet-50 backbone that was fine tuned. In other words, this closely aligns with the 'pretrained encoder' with a decoder head methology that has been popularized in the last year.\n",
    "\n",
    "#### Linear Probe\n",
    "\n",
    "<img src=\"./assets/linear.png\" style='height:400px'/>\n",
    "\n",
    "Figure 8. Simplified structure of a single linear layer.\n",
    "\n",
    "The simplest decoder available. It is a single linear layer to process just the output of the encoder. In practice, this isn't typically used for segmentation due a limited model complexity. It is still useful, however, to assess representation quality as it is cheap to train and can be implemented in a consistently, making it an ideal tool for benchmarking. This technique is taken from the language domain where large language models are often evaluated on downstream tasks with linear probes on frozen embeddings before doing full fine-tuning. \n",
    "\n",
    "#### Muster\n",
    "\n",
    "<img src=\"./assets/muster.png\" style='height:400px'/>\n",
    "\n",
    "Figure 9. Architecural diagram of MUSTER decoder.\n",
    "\n",
    "A more recent decoder development in the computer vision realm. Similar to both the Segformer and the UperNet, hierarchical features play an important role in it's predictive strength with the most important distinction here being that the decoder itself is also transformer based. Also similar to the UperNet, the MUSTER decoder was designed to integrate with pretrained encoders. While this may lend it self to greater performance, it is important to consider the compute required to implement such an architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84a747",
   "metadata": {},
   "source": [
    "## Section B: The Framework\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "[Link to original repository](https://github.com/VMarsocci/pangaea-bench)\n",
    "\n",
    "We apply the Pangaea Bench benchmarking framework originally developed by the ESA Phi Lab team. While, there are other frameworks in development by other teams, Pangaea conveniently has many of the more popular remote sensing foundation models integrated into their pure pytorch framework along with a number of well established benchmarking datasets. It also includes a relatively simple dataset integration scheme along with additional decoder heads integrated by the team at Spatial Informatics Group for the purpose of evaluating methods for extracting the information from the internal representation of the foundation model. The limited dependency is especially beneficial for small scale demonstrations such as this learning notebook. You will find similar themes found in previous chapters of the Applied Deep Learning Book such as dataloaders, pytorch modules, loss functions, etc. These are not exclusiive to training end to end models and in fact, the workflow is almost exactly the same! The intended purpose of EOFMs is solving the same segmentation, classification, or regression problems. Here, we focus on the additional considerations that come with using an EOFM for segmentation.\n",
    "\n",
    "<img src=\"./assets/pangaea_workflow.png\" style='height:400px'/>\n",
    "\n",
    "Figure 5. Diagram of Pangaea's general structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Software/Hardware considerations\n",
    "\n",
    "CUDA is the underlying software running NVidia GPUs. While it is not necessary, we recommend running this notebook in a CUDA enabled linux environment for convenience and setup simplicity. We also recommend installing python>=3.10 either standalone or through a conda managed environment. For running the model, we can estimate a VRAM required for storing the model using a simple calculation of # of parameters and precision. For example, a 300M parameter model at 32bit precision would roughly require 3.6GB to store the weights and optimizer states. Another 1.2 GB is required for the gradients of each of those parameters and a few more for the data itself. Likewise, the decoder used to extract from the internal representation also imposes VRAM / compute requirements. There's no universal formula here, but tools like PyTorch hooks, torch.cuda.memory_summary(), or profilers can help estimate how much compute you need. 12GB of VRAM is a good place to start and luckily there are free google colab options at this compute scale, but, there is a trend towards larger and more complex models which inevitably consume more compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51d3be",
   "metadata": {},
   "source": [
    "#### Installing framework and requirements\n",
    "\n",
    "We are installing directly from a cloned repo along with the available requirements.txt file. This may take a few moments as PyTorch is a fairly large package. Please install python and pip on your own and setup on your notebook environment that best suits your preferences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0400c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pangaea-bench' already exists and is not an empty directory.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///home/myscon/workstation/eofm-book/pangaea-bench\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Ignoring invalid distribution -andas (/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pangaea\n",
      "  Attempting uninstall: pangaea\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -andas (/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pangaea 1.0.0\n",
      "    Uninstalling pangaea-1.0.0:\n",
      "      Successfully uninstalled pangaea-1.0.0\n",
      "\u001b[33m  DEPRECATION: Legacy editable install of pangaea==1.0.0 from file:///home/myscon/workstation/eofm-book/pangaea-bench (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py develop for pangaea\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pangaea\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch==2.3.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: torchvision==0.18.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 2)) (0.18.0)\n",
      "Requirement already satisfied: geopandas in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: rasterio in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: pillow in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 5)) (11.0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: tensorboard in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 7)) (2.18.0)\n",
      "Requirement already satisfied: tqdm in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: tifffile in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 9)) (2020.6.3)\n",
      "Requirement already satisfied: timm in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 10)) (1.0.12)\n",
      "Requirement already satisfied: einops in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 11)) (0.8.0)\n",
      "Requirement already satisfied: opencv-python in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 12)) (4.12.0.88)\n",
      "Requirement already satisfied: imagecodecs in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 13)) (2025.3.30)\n",
      "Requirement already satisfied: rioxarray in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 14)) (0.17.0)\n",
      "Requirement already satisfied: gdown in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 15)) (5.2.0)\n",
      "Requirement already satisfied: ptflops in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 16)) (0.7.3)\n",
      "Requirement already satisfied: google-cloud-storage in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 17)) (2.19.0)\n",
      "Requirement already satisfied: omegaconf in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 18)) (2.3.0)\n",
      "Requirement already satisfied: pyDataverse in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 19)) (0.3.3)\n",
      "Requirement already satisfied: pytest in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 20)) (8.3.4)\n",
      "Requirement already satisfied: yacs in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 21)) (0.1.8)\n",
      "Requirement already satisfied: wandb in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 22)) (0.19.1)\n",
      "Requirement already satisfied: hydra-core in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 23)) (1.3.2)\n",
      "Requirement already satisfied: wheel in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from -r pangaea-bench/requirements.txt (line 24)) (0.45.1)\n",
      "Requirement already satisfied: filelock in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from torchvision==0.18.0->-r pangaea-bench/requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (12.9.86)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from geopandas->-r pangaea-bench/requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: packaging in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from geopandas->-r pangaea-bench/requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from geopandas->-r pangaea-bench/requirements.txt (line 3)) (2.3.1)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from geopandas->-r pangaea-bench/requirements.txt (line 3)) (3.7.0)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from geopandas->-r pangaea-bench/requirements.txt (line 3)) (2.0.6)\n",
      "Requirement already satisfied: affine in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from rasterio->-r pangaea-bench/requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: attrs in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from rasterio->-r pangaea-bench/requirements.txt (line 4)) (24.3.0)\n",
      "Requirement already satisfied: certifi in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from rasterio->-r pangaea-bench/requirements.txt (line 4)) (2025.7.9)\n",
      "Requirement already satisfied: click>=4.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from rasterio->-r pangaea-bench/requirements.txt (line 4)) (8.1.8)\n",
      "Requirement already satisfied: cligj>=0.5 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from rasterio->-r pangaea-bench/requirements.txt (line 4)) (0.7.2)\n",
      "Requirement already satisfied: click-plugins in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from rasterio->-r pangaea-bench/requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from rasterio->-r pangaea-bench/requirements.txt (line 4)) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from scikit-learn->-r pangaea-bench/requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from scikit-learn->-r pangaea-bench/requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from scikit-learn->-r pangaea-bench/requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from tensorboard->-r pangaea-bench/requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from tensorboard->-r pangaea-bench/requirements.txt (line 7)) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from tensorboard->-r pangaea-bench/requirements.txt (line 7)) (3.6)\n",
      "Requirement already satisfied: protobuf in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from tensorboard->-r pangaea-bench/requirements.txt (line 7)) (5.28.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from tensorboard->-r pangaea-bench/requirements.txt (line 7)) (75.6.0)\n",
      "Requirement already satisfied: six>1.9 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from tensorboard->-r pangaea-bench/requirements.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from tensorboard->-r pangaea-bench/requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from tensorboard->-r pangaea-bench/requirements.txt (line 7)) (3.1.3)\n",
      "Requirement already satisfied: pyyaml in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from timm->-r pangaea-bench/requirements.txt (line 10)) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from timm->-r pangaea-bench/requirements.txt (line 10)) (0.19.4)\n",
      "Requirement already satisfied: safetensors in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from timm->-r pangaea-bench/requirements.txt (line 10)) (0.4.5)\n",
      "Requirement already satisfied: xarray>=2022.3.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from rioxarray->-r pangaea-bench/requirements.txt (line 14)) (2024.11.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from gdown->-r pangaea-bench/requirements.txt (line 15)) (4.12.3)\n",
      "Requirement already satisfied: requests[socks] in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from gdown->-r pangaea-bench/requirements.txt (line 15)) (2.32.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (2.37.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (2.24.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (1.1.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from omegaconf->-r pangaea-bench/requirements.txt (line 18)) (4.9.3)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (0.27.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (4.23.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pytest->-r pangaea-bench/requirements.txt (line 20)) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pytest->-r pangaea-bench/requirements.txt (line 20)) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pytest->-r pangaea-bench/requirements.txt (line 20)) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pytest->-r pangaea-bench/requirements.txt (line 20)) (2.2.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from wandb->-r pangaea-bench/requirements.txt (line 22)) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from wandb->-r pangaea-bench/requirements.txt (line 22)) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from wandb->-r pangaea-bench/requirements.txt (line 22)) (4.3.6)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from wandb->-r pangaea-bench/requirements.txt (line 22)) (6.1.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from wandb->-r pangaea-bench/requirements.txt (line 22)) (2.10.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from wandb->-r pangaea-bench/requirements.txt (line 22)) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from wandb->-r pangaea-bench/requirements.txt (line 22)) (1.3.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r pangaea-bench/requirements.txt (line 22)) (4.0.11)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (1.66.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (1.25.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (4.9)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from google-crc32c<2.0dev,>=1.0->google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (1.17.1)\n",
      "Requirement already satisfied: anyio in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.21.1->pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.21.1->pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.21.1->pyDataverse->-r pangaea-bench/requirements.txt (line 19)) (0.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas->-r pangaea-bench/requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas->-r pangaea-bench/requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas->-r pangaea-bench/requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb->-r pangaea-bench/requirements.txt (line 22)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb->-r pangaea-bench/requirements.txt (line 22)) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from requests[socks]->gdown->-r pangaea-bench/requirements.txt (line 15)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from requests[socks]->gdown->-r pangaea-bench/requirements.txt (line 15)) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r pangaea-bench/requirements.txt (line 7)) (3.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from beautifulsoup4->gdown->-r pangaea-bench/requirements.txt (line 15)) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from requests[socks]->gdown->-r pangaea-bench/requirements.txt (line 15)) (1.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from sympy->torch==2.3.0->-r pangaea-bench/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (2.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r pangaea-bench/requirements.txt (line 22)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r pangaea-bench/requirements.txt (line 17)) (0.6.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sig-gis/pangaea-bench.git \"pangaea-bench\"\n",
    "!pip install -e \"pangaea-bench\"\n",
    "!pip install -r pangaea-bench/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6ea02",
   "metadata": {},
   "source": [
    "#### The 'torchrun' Command\n",
    "\n",
    "The torchrun command is a utility provided by PyTorch to launch distributed training jobs across multiple processes, nodes, or GPUs. It replaces the older torch.distributed.launch and is part of the torch.distributed module. torchrun is designed to be simple and flexible, making it easier to scale up training scripts with minimal changes. At its core, torchrun sets up the environment variables necessary for distributed training, such as RANK, WORLD_SIZE, and MASTER_ADDR, and then spawns multiple processes, one per GPU or per node, depending on the configuration. This enables parallel training using frameworks like DistributedDataParallel (DDP), which helps synchronize gradients and reduce training time significantly. This will be especially useful for training larger networks however, for the purposes of this demonstration we will only use it to run on a single machine\n",
    "\n",
    "The Pangaea benchmarking framework is a lightweight wrappper around this command to handle most of boilerplate code that comes with training a neural network. This includes establishing the training loop, calculating metrics, logging, as well as the datasets/dataloaders associated with the benchmarks. [See torchrun documentation on environment variables for more details](https://docs.pytorch.org/docs/stable/elastic/run.html). There are several command options available which can be found in ./pangaea-bench/configs. We only use a subset below in this demonstration notebook but there are many more available. Config files associated with the dataset, decoder, and encoder are associated with a PyTorch module that is instantiated by the framework based on the configurations provided.\n",
    "\n",
    "```bash\n",
    "!torchrun pangaea-bench/pangaea/run.py \\                                ##### The torchrun entry command\n",
    "    --config-name=train \\                                               ##### configuration name which can be found in ./pangaea-bench/configs\n",
    "    work_dir=checkpoints \\                                              ##### the directory relative to the working directory to store checkpoint outputs\n",
    "    dataset=hlsburnscars \\                                              ##### the dataset config file found in ./pangaea-bench/configs\n",
    "    encoder=dofa\\                                                       ##### the encoder config file found in ./pangaea-bench/configs\n",
    "    decoder=seg_fcn\\                                                    ##### the decoder config file found in ./pangaea-bench/configs\n",
    "    preprocessing=seg_default\\                                          ##### preprocssing steps associated with the task type (e.g normalizing images)\n",
    "    criterion=cross_entropy \\                                           ##### the loss function on which to optimize the training cycle\n",
    "    task=segmentation \\                                                 ##### task type (others in clude classification and regression) \n",
    "    use_wandb=true \\                                                    ##### whether to use the online logging software\n",
    "    task.trainer.n_epochs=16 \\                                          ##### number of epochs to limit training\n",
    "    task.trainer.log_interval=1 \\                                       ##### logging interval of calculated metrics\n",
    "    task.trainer.ckpt_interval=16 \\                                     ##### how often to save checkpoints based on metrics\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pth    ##### path to pretarined weights of the FM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5536b8",
   "metadata": {},
   "source": [
    "#### Logging and Model Examination\n",
    "\n",
    "Weights & Biases (WandB) is a popular tool for experiment tracking, model monitoring, and collaboration in machine learning workflows. It integrates seamlessly with PyTorch, TensorFlow, Keras, and other frameworks, allowing users to log training metrics, visualize results in real-time, and compare model performance across runs. Pangaea benchmark happens to have a prebaked integration of wandb which is easily accessed using WandB's authentication protocol. [See WandD documentation on environment variables for more details](https://docs.wandb.ai/guides/track/environment-variables/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ab794",
   "metadata": {},
   "source": [
    "## Section C: Example Workflow: Burn Scar Mapping (MTBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02382640",
   "metadata": {},
   "source": [
    "### Frozen vs Unfrozen vs Random vs LoRA\n",
    "\n",
    "Testing frozen, unfrozen, and randomly initialized pretrained encoders is useful in understanding the value and applicability of transfer learning for a specific task. A frozen encoder uses pretrained weights without updating them during training. This setup helps evaluate how useful the pretrained features are on their own, especially in cases where the target dataset is small or the model is prone to overfitting. In contrast, an unfrozen encoder allows those pretrained weights to be updated, enabling the model to adapt its learned representations to better suit the target task. This approach often yields better performance when sufficient labeled data is available and the task deviates from the original pretraining domain however, requires that additional gradients be computed and stored. On the other hand, a randomly initialized encoder serves as a baseline, providing a measure of how well the model performs without any prior knowledge. Comparing results from this setup against those using pretrained weights helps quantify the benefits of pretraining in terms of training time and overall final performance. It's worth noting that depending on the task and the amount of available training data, the randomly initialized encoder may never achieve the same results as an unfrozen pretrained model. This behavior is still an on going area of research. \n",
    "\n",
    "Overall, these three basic configurations allow for a comprehensive assessment of whether transfer learning is useful, whether fine-tuning improves results, and whether pretraining is necessary at all for a specific task. There is a fourth method called Low Rank Adaption (LoRA) which is somewhat of a compromise between a fully frozen and unfrozen encoder. For larger models (i.e greater than 300m parameters). Low-Rank Adaptation (LoRA) is a technique used to efficiently fine-tune large pretrained neural networks by injecting small, trainable weight matrices into the model, while keeping the original weights frozen. Traditional fine-tuning updates all parameters of a model, which becomes expensive for large models. LoRA avoids this by approximating the weight updates as the product of two low-rank matrices, significantly reducing the number of trainable parameters. This low-rank decomposition acts as a bottleneck, enforcing efficiency and reducing overfitting. LoRA has gained popularity particularly with large language models. It also enables modular fine-tuning where separate LoRA modules can be swapped in for different tasks—making it attractive for applications requiring task specialization without retraining the entire model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e329355",
   "metadata": {},
   "source": [
    "### Lets Fine Tune!\n",
    "\n",
    "Okay, we've established some of the basic concepts, lets train some models! We'll be focusing on a single task for 3 frozen encoder variations: DOFA, CROMA, and Prithvi 1.0 (described below). Along with those 3 FMs we train the 4 different decoders described in section A. That's 12 different models for just a single task! It's clear here that for a comparison analysis for foundation models, compute becomes a key bottleneck in research especially when aiming for systematic comparisons across models, tasks, or training regimes. For the sake of this demonstration, we limit each training run to a batch size of 8 for 16 epochs an on a single RTX 4000 workstation GPU with 12GB of VRAM. A machine of this size is readily available on any cloud compute platform.\n",
    "\n",
    "The task we are targeting is segementing burn scars in HLS scenes. An HLS-based benchmark is unique in that it sources its optical data from two satellite missions (Landsat and Sentinel) and in doing so, provides a relatively level test bed for models trained on either. Nevertheless, it’s important to acknowledge the nature of the benchmark dataset used. While it has been carefully curated to support robust model evaluation, it may not fully reflect the diversity or complexity of real-world fire scenarios with potentially with drastically different landscape dynamics, data availability, and task scope. The dataset is limited in geographic scope, vegetation types, and imaging conditions, which means performance metrics obtained here may not generalize well to all fire contexts such as risk mapping. As such, results from this benchmark should be interpreted with consideration of these constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d80bb",
   "metadata": {},
   "source": [
    "#### DOFA\n",
    "[Link to paper](https://arxiv.org/pdf/2403.15356)\n",
    "\n",
    "DOFA is a transformer-based model that is optimized on both a mask image modeling objective and self distillation objective (Figure 2b). DOFA's major contribution is their wavelength-conditioned dynamic patch embedding in which a the central wavelength of a given sensor is used to derive the weights for processing the resepective data. It is apparent from this that their approach is targettiing flexibility and generalizability across multiple modalities and does so fairly well. And yet, there are still open questions! For example, wavelength and reflectance does not necessarily translate to amplitude data from SAR. Does this matter? Benchmarking metrics will tell you no but do keep in mind: why?\n",
    "\n",
    "<img src=\"./assets/dofa.png\"/>\n",
    "\n",
    "Figure 6: (a) Architecture design. DOFA builds on masked image modeling by processing input images with any number of channels within a single framework. (b) Dynamic\n",
    "weight generator and continual training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmtruong\u001b[0m (\u001b[33mspatial-informatics-group\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/myscon/workstation/eofm-book/wandb/run-20250715_155655-y9eilllh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m20250715_155654_fc10d0_dofa_seg_fcn_hlsburnscars\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/spatial-informatics-group/geofm-bench\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/spatial-informatics-group/geofm-bench/runs/y9eilllh\u001b[0m\n",
      "INFO - 07/15/25 15:56:55 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 07/15/25 15:56:55 - 0:00:00 - 'batch_size': 8,\n",
      "                                      'ckpt_dir': None,\n",
      "                                      'criterion': {'_target_': 'torch.nn.CrossEntropyLoss',\n",
      "                                                    'ignore_index': '${dataset.ignore_index}'},\n",
      "                                      'data_replicate': 1,\n",
      "                                      'dataset': {'_target_': 'pangaea.datasets.hlsburnscars.HLSBurnScars',\n",
      "                                                  'auto_download': True,\n",
      "                                                  'bands': {'optical': ['B2', 'B3', 'B4', 'B8A', 'B11', 'B12']},\n",
      "                                                  'classes': ['Not burned', 'Burn scar'],\n",
      "                                                  'data_max': {'optical': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
      "                                                  'data_mean': {'optical': [0.033349706741586264,\n",
      "                                                                            0.05701185520536176, 0.05889748132001316,\n",
      "                                                                            0.2323245113436119, 0.1972854853760658,\n",
      "                                                                            0.11944914225186566]},\n",
      "                                                  'data_min': {'optical': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
      "                                                  'data_std': {'optical': [0.02269135568823774, 0.026807560223070237,\n",
      "                                                                           0.04004109844362779, 0.07791732423672691,\n",
      "                                                                           0.08708738838140137,\n",
      "                                                                           0.07241979477437814]},\n",
      "                                                  'dataset_name': 'HLSBurnScars',\n",
      "                                                  'distribution': [0.88889, 0.11111],\n",
      "                                                  'download_url': 'https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars/resolve/main/hls_burn_scars.tar.gz?download=true',\n",
      "                                                  'ignore_index': -1,\n",
      "                                                  'img_size': 512,\n",
      "                                                  'multi_modal': False,\n",
      "                                                  'multi_temporal': False,\n",
      "                                                  'num_classes': 2,\n",
      "                                                  'root_path': './data/hlsburnscars'},\n",
      "                                      'decoder': {'_target_': 'pangaea.decoders.fcn.FCNDecoder',\n",
      "                                                  'channels': 1024,\n",
      "                                                  'concat_input': True,\n",
      "                                                  'dilation': 1,\n",
      "                                                  'encoder': None,\n",
      "                                                  'finetune': '${finetune}',\n",
      "                                                  'interp_method': 'PixelShuffle',\n",
      "                                                  'kernel_size': 3,\n",
      "                                                  'num_classes': '${dataset.num_classes}',\n",
      "                                                  'num_convs': 4},\n",
      "                                      'emebed': True,\n",
      "                                      'encoder': {'_target_': 'pangaea.encoders.dofa_encoder.DOFA_Encoder',\n",
      "                                                  'depth': 12,\n",
      "                                                  'download_url': 'https://huggingface.co/XShadow/DOFA/resolve/main/DOFA_ViT_base_e100.pth',\n",
      "                                                  'embed_dim': 768,\n",
      "                                                  'encoder_weights': 'pretrained_models/DOFA_ViT_base_e100.pth',\n",
      "                                                  'input_bands': '${dataset.bands}',\n",
      "                                                  'input_size': 224,\n",
      "                                                  'mlp_ratio': 4,\n",
      "                                                  'num_heads': 12,\n",
      "                                                  'output_dim': 768,\n",
      "                                                  'output_layers': [3, 5, 7, 11],\n",
      "                                                  'patch_size': 16,\n",
      "                                                  'use_norm': False,\n",
      "                                                  'wave_list': {'optical': {'B1': 0.44,\n",
      "                                                                            'B10': 1.373,\n",
      "                                                                            'B11': 1.61,\n",
      "                                                                            'B12': 2.2,\n",
      "                                                                            'B2': 0.49,\n",
      "                                                                            'B3': 0.56,\n",
      "                                                                            'B4': 0.665,\n",
      "                                                                            'B5': 0.705,\n",
      "                                                                            'B6': 0.74,\n",
      "                                                                            'B7': 0.783,\n",
      "                                                                            'B8': 0.832,\n",
      "                                                                            'B8A': 0.864,\n",
      "                                                                            'B9': 0.945},\n",
      "                                                                'sar': {'ASC_VH': 3.75,\n",
      "                                                                        'ASC_VV': 3.75,\n",
      "                                                                        'DSC_VH': 3.75,\n",
      "                                                                        'DSC_VV': 3.75,\n",
      "                                                                        'VH': 3.75,\n",
      "                                                                        'VV': 3.75,\n",
      "                                                                        'VV-VH': 3.75}}},\n",
      "                                      'finetune': False,\n",
      "                                      'limited_label_strategy': 'stratified',\n",
      "                                      'limited_label_train': 1,\n",
      "                                      'limited_label_val': 1,\n",
      "                                      'lr_scheduler': {'_target_': 'pangaea.utils.schedulers.MultiStepLR',\n",
      "                                                       'lr_milestones': [0.6, 0.9],\n",
      "                                                       'optimizer': None,\n",
      "                                                       'total_iters': None},\n",
      "                                      'num_workers': 0,\n",
      "                                      'optimizer': {'_target_': 'torch.optim.AdamW',\n",
      "                                                    'betas': [0.9, 0.999],\n",
      "                                                    'lr': 0.0001,\n",
      "                                                    'params': None,\n",
      "                                                    'weight_decay': 0.05},\n",
      "                                      'preprocessing': {'test': {'_target_': 'pangaea.engine.data_preprocessor.Preprocessor',\n",
      "                                                                 'preprocessor_cfg': [{'_target_': 'pangaea.engine.data_preprocessor.BandFilter'},\n",
      "                                                                                      {'_target_': 'pangaea.engine.data_preprocessor.NormalizeMeanStd'},\n",
      "                                                                                      {'_target_': 'pangaea.engine.data_preprocessor.BandPadding'}]},\n",
      "                                                        'train': {'_target_': 'pangaea.engine.data_preprocessor.Preprocessor',\n",
      "                                                                  'preprocessor_cfg': [{'_target_': 'pangaea.engine.data_preprocessor.RandomCropToEncoder'},\n",
      "                                                                                       {'_target_': 'pangaea.engine.data_preprocessor.BandFilter'},\n",
      "                                                                                       {'_target_': 'pangaea.engine.data_preprocessor.NormalizeMeanStd'},\n",
      "                                                                                       {'_target_': 'pangaea.engine.data_preprocessor.BandPadding'}]},\n",
      "                                                        'val': {'_target_': 'pangaea.engine.data_preprocessor.Preprocessor',\n",
      "                                                                'preprocessor_cfg': [{'_target_': 'pangaea.engine.data_preprocessor.BandFilter'},\n",
      "                                                                                     {'_target_': 'pangaea.engine.data_preprocessor.NormalizeMeanStd'},\n",
      "                                                                                     {'_target_': 'pangaea.engine.data_preprocessor.BandPadding'}]}},\n",
      "                                      'seed': 234,\n",
      "                                      'stratification_bins': 3,\n",
      "                                      'task': {'evaluator': {'_target_': 'pangaea.engine.evaluator.SegEvaluator',\n",
      "                                                             'device': None,\n",
      "                                                             'exp_dir': None,\n",
      "                                                             'inference_mode': 'sliding',\n",
      "                                                             'sliding_inference_batch': 8,\n",
      "                                                             'use_wandb': '${use_wandb}',\n",
      "                                                             'val_loader': None},\n",
      "                                               'trainer': {'_target_': 'pangaea.engine.trainer.SegTrainer',\n",
      "                                                           'best_metric_key': 'mIoU',\n",
      "                                                           'ckpt_interval': 16,\n",
      "                                                           'criterion': None,\n",
      "                                                           'device': None,\n",
      "                                                           'eval_interval': 5,\n",
      "                                                           'evaluator': None,\n",
      "                                                           'exp_dir': None,\n",
      "                                                           'log_interval': 1,\n",
      "                                                           'lr_scheduler': None,\n",
      "                                                           'model': None,\n",
      "                                                           'n_epochs': 16,\n",
      "                                                           'optimizer': None,\n",
      "                                                           'precision': 'fp32',\n",
      "                                                           'train_loader': None,\n",
      "                                                           'use_wandb': '${use_wandb}'}},\n",
      "                                      'test_batch_size': 1,\n",
      "                                      'test_num_workers': 0,\n",
      "                                      'train': True,\n",
      "                                      'use_final_ckpt': False,\n",
      "                                      'use_wandb': True,\n",
      "                                      'wandb_run_id': 'y9eilllh',\n",
      "                                      'work_dir': 'checkpoints'\n",
      "INFO - 07/15/25 15:56:55 - 0:00:00 - The experiment is stored in checkpoints/20250715_155654_fc10d0_dofa_seg_fcn_hlsburnscars\n",
      "                                     \n",
      "INFO - 07/15/25 15:56:55 - 0:00:00 - Device used: cuda:0\n",
      "INFO - 07/15/25 15:56:56 - 0:00:01 - Built dofa_encoder.\n",
      "INFO - 07/15/25 15:56:56 - 0:00:01 - Encoder Parameter Count: 111,312,128\n",
      "/home/myscon/miniconda3/envs/pangaea-bench/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "INFO - 07/15/25 15:56:57 - 0:00:02 - Built FCN for with DOFA_Encoder encoder.\n",
      "INFO - 07/15/25 15:56:57 - 0:00:02 - Decoder Parameter Count: 52,439,552\n",
      "INFO - 07/15/25 15:56:57 - 0:00:02 - Built HLSBurnScars dataset.\n",
      "INFO - 07/15/25 15:56:57 - 0:00:02 - Total number of train patches: 486\n",
      "                                     Total number of validation patches: 54\n",
      "                                     \n",
      "Evaluating epoch 0 on val set: 100%|████████████| 54/54 [00:02<00:00, 18.77it/s]\n",
      "INFO - 07/15/25 15:57:00 - 0:00:05 - [val] ------- IoU --------\n",
      "                                     Not burned\t 54.775\n",
      "                                     Burn scar \t  9.310\n",
      "                                     [val]-------------------\n",
      "                                     [val] Mean\t 32.042\n",
      "INFO - 07/15/25 15:57:00 - 0:00:05 - [val] ------- F1-score --------\n",
      "                                     Not burned\t 70.780\n",
      "                                     Burn scar \t 17.034\n",
      "                                     [val]-------------------\n",
      "                                     [val] Mean\t 43.907\n",
      "INFO - 07/15/25 15:57:00 - 0:00:05 - [val] ------- Precision --------\n",
      "                                     Not burned\t 58.255\n",
      "                                     Burn scar \t 43.725\n",
      "                                     [val]-------------------\n",
      "                                     [val] Mean\t 50.990\n",
      "INFO - 07/15/25 15:57:00 - 0:00:05 - [val] ------- Recall --------\n",
      "                                     Not burned\t 90.164\n",
      "                                     Burn scar \t 10.577\n",
      "                                     [val]-------------------\n",
      "                                     [val] Mean\t 50.371\n",
      "INFO - 07/15/25 15:57:00 - 0:00:05 - Mean Accuracy: 56.781 \n",
      "                                     \n",
      "INFO - 07/15/25 15:57:00 - 0:00:05 - Epoch 0 | Training checkpoint saved at checkpoints/20250715_155654_fc10d0_dofa_seg_fcn_hlsburnscars/checkpoint__best.pth\n",
      "INFO - 07/15/25 15:57:00 - 0:00:05 - ============ Starting epoch 0 ... ============\n",
      "INFO - 07/15/25 15:57:01 - 0:00:06 - Epoch [0-1/60]\tETA [00h00m11s|00h00m00s]\tTime [0.000|0.033]\tLoss 0.7055 (0.7055)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  53.509 ( 53.509)\tmAcc  50.553 ( 50.553)\tmIoU  34.632 ( 34.632)\n",
      "INFO - 07/15/25 15:57:01 - 0:00:06 - Epoch [0-2/60]\tETA [00h06m24s|00h00m22s]\tTime [0.389|0.038]\tLoss 0.6972 (0.7013)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  59.672 ( 56.591)\tmAcc  52.141 ( 51.347)\tmIoU  34.816 ( 34.724)\n",
      "INFO - 07/15/25 15:57:01 - 0:00:06 - Epoch [0-3/60]\tETA [00h04m05s|00h00m13s]\tTime [0.244|0.033]\tLoss 0.6830 (0.6952)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  65.726 ( 59.636)\tmAcc  53.153 ( 51.949)\tmIoU  37.981 ( 35.810)\n",
      "INFO - 07/15/25 15:57:01 - 0:00:06 - Epoch [0-4/60]\tETA [00h03m12s|00h00m10s]\tTime [0.189|0.033]\tLoss 0.6604 (0.6865)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  64.491 ( 60.850)\tmAcc  57.898 ( 53.436)\tmIoU  42.798 ( 37.557)\n",
      "INFO - 07/15/25 15:57:01 - 0:00:06 - Epoch [0-5/60]\tETA [00h02m49s|00h00m09s]\tTime [0.165|0.035]\tLoss 0.6481 (0.6788)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  69.011 ( 62.482)\tmAcc  58.092 ( 54.367)\tmIoU  41.239 ( 38.293)\n",
      "INFO - 07/15/25 15:57:01 - 0:00:06 - Epoch [0-6/60]\tETA [00h02m36s|00h00m08s]\tTime [0.152|0.034]\tLoss 0.6604 (0.6757)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  65.252 ( 62.943)\tmAcc  54.978 ( 54.469)\tmIoU  40.889 ( 38.726)\n",
      "INFO - 07/15/25 15:57:01 - 0:00:06 - Epoch [0-7/60]\tETA [00h02m26s|00h00m07s]\tTime [0.141|0.033]\tLoss 0.6481 (0.6718)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  66.772 ( 63.490)\tmAcc  58.042 ( 54.980)\tmIoU  43.664 ( 39.431)\n",
      "INFO - 07/15/25 15:57:01 - 0:00:06 - Epoch [0-8/60]\tETA [00h02m18s|00h00m06s]\tTime [0.133|0.032]\tLoss 0.6096 (0.6640)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.731 ( 64.895)\tmAcc  59.705 ( 55.570)\tmIoU  47.324 ( 40.418)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:06 - Epoch [0-9/60]\tETA [00h02m12s|00h00m06s]\tTime [0.127|0.041]\tLoss 0.6516 (0.6626)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  65.972 ( 65.015)\tmAcc  49.727 ( 54.921)\tmIoU  37.904 ( 40.139)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-10/60]\tETA [00h02m16s|00h00m06s]\tTime [0.132|0.040]\tLoss 0.5969 (0.6561)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.911 ( 66.105)\tmAcc  53.885 ( 54.817)\tmIoU  43.126 ( 40.437)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-11/60]\tETA [00h02m12s|00h00m06s]\tTime [0.127|0.039]\tLoss 0.6186 (0.6527)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  63.036 ( 65.826)\tmAcc  56.720 ( 54.990)\tmIoU  38.990 ( 40.306)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-12/60]\tETA [00h02m08s|00h00m05s]\tTime [0.124|0.039]\tLoss 0.5957 (0.6479)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.838 ( 66.660)\tmAcc  56.039 ( 55.078)\tmIoU  45.268 ( 40.719)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-13/60]\tETA [00h02m06s|00h00m05s]\tTime [0.121|0.038]\tLoss 0.6062 (0.6447)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  72.971 ( 67.145)\tmAcc  55.369 ( 55.100)\tmIoU  43.887 ( 40.963)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-14/60]\tETA [00h02m03s|00h00m05s]\tTime [0.118|0.037]\tLoss 0.5644 (0.6390)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.986 ( 68.420)\tmAcc  55.459 ( 55.126)\tmIoU  47.108 ( 41.402)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-15/60]\tETA [00h02m01s|00h00m05s]\tTime [0.116|0.037]\tLoss 0.5814 (0.6351)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.886 ( 69.451)\tmAcc  51.570 ( 54.889)\tmIoU  45.039 ( 41.644)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-16/60]\tETA [00h01m59s|00h00m05s]\tTime [0.114|0.036]\tLoss 0.5642 (0.6307)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.140 ( 69.744)\tmAcc  57.613 ( 55.059)\tmIoU  45.355 ( 41.876)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-17/60]\tETA [00h01m57s|00h00m04s]\tTime [0.112|0.039]\tLoss 0.5347 (0.6250)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.348 ( 70.309)\tmAcc  55.865 ( 55.106)\tmIoU  46.356 ( 42.140)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-18/60]\tETA [00h01m58s|00h00m04s]\tTime [0.114|0.039]\tLoss 0.5397 (0.6203)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.501 ( 71.208)\tmAcc  54.408 ( 55.068)\tmIoU  48.039 ( 42.468)\n",
      "INFO - 07/15/25 15:57:02 - 0:00:07 - Epoch [0-19/60]\tETA [00h01m58s|00h00m04s]\tTime [0.114|0.039]\tLoss 0.5437 (0.6163)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  73.458 ( 71.327)\tmAcc  56.989 ( 55.169)\tmIoU  43.576 ( 42.526)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:07 - Epoch [0-20/60]\tETA [00h01m57s|00h00m04s]\tTime [0.113|0.038]\tLoss 0.5093 (0.6109)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.986 ( 71.910)\tmAcc  57.541 ( 55.287)\tmIoU  49.180 ( 42.859)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-21/60]\tETA [00h01m55s|00h00m04s]\tTime [0.111|0.038]\tLoss 0.5286 (0.6070)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.366 ( 72.598)\tmAcc  53.805 ( 55.217)\tmIoU  47.180 ( 43.064)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-22/60]\tETA [00h01m54s|00h00m04s]\tTime [0.110|0.039]\tLoss 0.5691 (0.6053)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.457 ( 72.683)\tmAcc  54.417 ( 55.180)\tmIoU  42.010 ( 43.016)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-23/60]\tETA [00h01m54s|00h00m04s]\tTime [0.110|0.038]\tLoss 0.5271 (0.6019)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.115 ( 73.180)\tmAcc  53.966 ( 55.128)\tmIoU  46.782 ( 43.180)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-24/60]\tETA [00h01m53s|00h00m03s]\tTime [0.109|0.038]\tLoss 0.4957 (0.5975)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  76.441 ( 73.316)\tmAcc  59.398 ( 55.306)\tmIoU  47.553 ( 43.362)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-25/60]\tETA [00h01m52s|00h00m03s]\tTime [0.108|0.037]\tLoss 0.4957 (0.5934)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.239 ( 73.753)\tmAcc  58.257 ( 55.424)\tmIoU  50.315 ( 43.640)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-26/60]\tETA [00h01m51s|00h00m03s]\tTime [0.107|0.037]\tLoss 0.5927 (0.5934)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  64.785 ( 73.408)\tmAcc  54.788 ( 55.399)\tmIoU  38.430 ( 43.440)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-27/60]\tETA [00h01m50s|00h00m03s]\tTime [0.106|0.037]\tLoss 0.5246 (0.5908)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.926 ( 73.723)\tmAcc  50.402 ( 55.214)\tmIoU  43.655 ( 43.448)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-28/60]\tETA [00h01m49s|00h00m03s]\tTime [0.105|0.037]\tLoss 0.5268 (0.5885)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  65.354 ( 73.424)\tmAcc  55.152 ( 55.212)\tmIoU  38.733 ( 43.280)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-29/60]\tETA [00h01m49s|00h00m03s]\tTime [0.105|0.037]\tLoss 0.5096 (0.5858)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  78.723 ( 73.607)\tmAcc  60.185 ( 55.383)\tmIoU  49.552 ( 43.496)\n",
      "INFO - 07/15/25 15:57:03 - 0:00:08 - Epoch [0-30/60]\tETA [00h01m48s|00h00m03s]\tTime [0.104|0.037]\tLoss 0.5262 (0.5838)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.713 ( 73.810)\tmAcc  58.724 ( 55.495)\tmIoU  48.792 ( 43.673)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:08 - Epoch [0-31/60]\tETA [00h01m48s|00h00m03s]\tTime [0.104|0.037]\tLoss 0.5055 (0.5813)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.330 ( 74.021)\tmAcc  57.603 ( 55.563)\tmIoU  48.172 ( 43.818)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-32/60]\tETA [00h01m47s|00h00m02s]\tTime [0.104|0.036]\tLoss 0.4618 (0.5776)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.303 ( 74.217)\tmAcc  57.567 ( 55.625)\tmIoU  48.116 ( 43.952)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-33/60]\tETA [00h01m46s|00h00m02s]\tTime [0.103|0.036]\tLoss 0.5307 (0.5761)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  67.084 ( 74.001)\tmAcc  58.800 ( 55.722)\tmIoU  42.011 ( 43.893)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-34/60]\tETA [00h01m46s|00h00m02s]\tTime [0.103|0.036]\tLoss 0.4912 (0.5736)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.555 ( 74.164)\tmAcc  59.603 ( 55.836)\tmIoU  49.486 ( 44.058)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-35/60]\tETA [00h01m46s|00h00m02s]\tTime [0.102|0.036]\tLoss 0.6559 (0.5760)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  71.126 ( 74.077)\tmAcc  49.059 ( 55.642)\tmIoU  38.400 ( 43.896)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-36/60]\tETA [00h01m45s|00h00m02s]\tTime [0.102|0.036]\tLoss 0.4536 (0.5726)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.508 ( 74.284)\tmAcc  60.305 ( 55.772)\tmIoU  50.873 ( 44.090)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-37/60]\tETA [00h01m45s|00h00m02s]\tTime [0.102|0.035]\tLoss 0.4636 (0.5696)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.770 ( 74.297)\tmAcc  58.970 ( 55.858)\tmIoU  45.993 ( 44.141)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-38/60]\tETA [00h01m44s|00h00m02s]\tTime [0.101|0.035]\tLoss 0.4413 (0.5663)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.361 ( 74.325)\tmAcc  61.054 ( 55.995)\tmIoU  48.029 ( 44.244)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-39/60]\tETA [00h01m44s|00h00m02s]\tTime [0.101|0.035]\tLoss 0.5450 (0.5657)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.759 ( 74.336)\tmAcc  51.175 ( 55.871)\tmIoU  40.840 ( 44.156)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-40/60]\tETA [00h01m43s|00h00m02s]\tTime [0.100|0.035]\tLoss 0.5001 (0.5641)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.883 ( 74.575)\tmAcc  54.311 ( 55.832)\tmIoU  46.949 ( 44.226)\n",
      "INFO - 07/15/25 15:57:04 - 0:00:09 - Epoch [0-41/60]\tETA [00h01m43s|00h00m01s]\tTime [0.100|0.035]\tLoss 0.4588 (0.5615)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  77.221 ( 74.639)\tmAcc  55.890 ( 55.834)\tmIoU  45.615 ( 44.260)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:09 - Epoch [0-42/60]\tETA [00h01m42s|00h00m01s]\tTime [0.100|0.035]\tLoss 0.5109 (0.5603)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.668 ( 74.664)\tmAcc  56.500 ( 55.850)\tmIoU  44.511 ( 44.266)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-43/60]\tETA [00h01m42s|00h00m01s]\tTime [0.099|0.035]\tLoss 0.5317 (0.5596)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  69.887 ( 74.553)\tmAcc  55.639 ( 55.845)\tmIoU  41.001 ( 44.190)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-44/60]\tETA [00h01m42s|00h00m01s]\tTime [0.099|0.034]\tLoss 0.5332 (0.5590)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  65.643 ( 74.350)\tmAcc  55.873 ( 55.845)\tmIoU  38.660 ( 44.064)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-45/60]\tETA [00h01m41s|00h00m01s]\tTime [0.099|0.034]\tLoss 0.4145 (0.5558)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.946 ( 74.586)\tmAcc  61.121 ( 55.963)\tmIoU  53.232 ( 44.268)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-46/60]\tETA [00h01m41s|00h00m01s]\tTime [0.099|0.034]\tLoss 0.4754 (0.5541)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  76.103 ( 74.619)\tmAcc  55.435 ( 55.951)\tmIoU  43.847 ( 44.259)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-47/60]\tETA [00h01m41s|00h00m01s]\tTime [0.098|0.034]\tLoss 0.6059 (0.5552)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  56.757 ( 74.239)\tmAcc  56.175 ( 55.956)\tmIoU  33.787 ( 44.036)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-48/60]\tETA [00h01m40s|00h00m01s]\tTime [0.098|0.034]\tLoss 0.4059 (0.5521)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.398 ( 74.492)\tmAcc  61.952 ( 56.081)\tmIoU  54.052 ( 44.245)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-49/60]\tETA [00h01m40s|00h00m01s]\tTime [0.098|0.034]\tLoss 0.4199 (0.5494)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  77.643 ( 74.556)\tmAcc  58.512 ( 56.130)\tmIoU  47.780 ( 44.317)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-50/60]\tETA [00h01m40s|00h00m00s]\tTime [0.098|0.034]\tLoss 0.6353 (0.5511)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  57.313 ( 74.211)\tmAcc  59.151 ( 56.191)\tmIoU  35.891 ( 44.148)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-51/60]\tETA [00h01m40s|00h00m00s]\tTime [0.097|0.034]\tLoss 0.5474 (0.5510)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  72.897 ( 74.186)\tmAcc  58.455 ( 56.235)\tmIoU  45.352 ( 44.172)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-52/60]\tETA [00h01m39s|00h00m00s]\tTime [0.097|0.034]\tLoss 0.3962 (0.5480)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.739 ( 74.369)\tmAcc  62.270 ( 56.351)\tmIoU  53.226 ( 44.346)\n",
      "INFO - 07/15/25 15:57:05 - 0:00:10 - Epoch [0-53/60]\tETA [00h01m39s|00h00m00s]\tTime [0.097|0.034]\tLoss 0.4133 (0.5455)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.516 ( 74.504)\tmAcc  58.637 ( 56.394)\tmIoU  49.476 ( 44.443)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:10 - Epoch [0-54/60]\tETA [00h01m39s|00h00m00s]\tTime [0.097|0.034]\tLoss 0.4428 (0.5436)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.824 ( 74.621)\tmAcc  62.646 ( 56.510)\tmIoU  52.288 ( 44.588)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [0-55/60]\tETA [00h01m39s|00h00m00s]\tTime [0.097|0.034]\tLoss 0.4226 (0.5414)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.755 ( 74.769)\tmAcc  58.905 ( 56.554)\tmIoU  49.074 ( 44.670)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [0-56/60]\tETA [00h01m39s|00h00m00s]\tTime [0.097|0.034]\tLoss 0.4220 (0.5393)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.176 ( 74.866)\tmAcc  61.978 ( 56.651)\tmIoU  51.698 ( 44.795)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [0-57/60]\tETA [00h01m38s|00h00m00s]\tTime [0.097|0.034]\tLoss 0.4099 (0.5370)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.901 ( 75.042)\tmAcc  62.563 ( 56.754)\tmIoU  53.906 ( 44.955)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [0-58/60]\tETA [00h01m38s|00h00m00s]\tTime [0.096|0.034]\tLoss 0.3976 (0.5346)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.849 ( 75.125)\tmAcc  60.249 ( 56.815)\tmIoU  50.138 ( 45.044)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [0-59/60]\tETA [00h01m38s|00h00m00s]\tTime [0.096|0.034]\tLoss 0.5609 (0.5350)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  73.793 ( 75.102)\tmAcc  49.318 ( 56.687)\tmIoU  38.872 ( 44.940)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [0-60/60]\tETA [00h01m38s|00h00m00s]\tTime [0.096|0.034]\tLoss 0.4119 (0.5330)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.426 ( 75.224)\tmAcc  62.558 ( 56.785)\tmIoU  53.285 ( 45.079)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - ============ Starting epoch 1 ... ============\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [1-1/60]\tETA [00h01m37s|00h00m05s]\tTime [0.096|0.033]\tLoss 0.3726 (0.5274)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.929 ( 75.432)\tmAcc  64.328 ( 56.909)\tmIoU  56.010 ( 45.258)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [1-2/60]\tETA [00h01m33s|00h00m05s]\tTime [0.091|0.033]\tLoss 0.4239 (0.5229)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.809 ( 75.616)\tmAcc  53.366 ( 56.852)\tmIoU  47.159 ( 45.289)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [1-3/60]\tETA [00h01m32s|00h00m05s]\tTime [0.091|0.033]\tLoss 0.5344 (0.5204)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  78.659 ( 75.664)\tmAcc  52.060 ( 56.776)\tmIoU  42.885 ( 45.251)\n",
      "INFO - 07/15/25 15:57:06 - 0:00:11 - Epoch [1-4/60]\tETA [00h01m32s|00h00m05s]\tTime [0.091|0.033]\tLoss 0.5431 (0.5185)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  67.933 ( 75.543)\tmAcc  57.995 ( 56.795)\tmIoU  41.211 ( 45.187)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:11 - Epoch [1-5/60]\tETA [00h01m32s|00h00m04s]\tTime [0.090|0.033]\tLoss 0.4063 (0.5144)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.363 ( 75.602)\tmAcc  57.969 ( 56.813)\tmIoU  47.324 ( 45.220)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:11 - Epoch [1-6/60]\tETA [00h01m32s|00h00m04s]\tTime [0.090|0.033]\tLoss 0.4074 (0.5102)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  77.896 ( 75.637)\tmAcc  62.006 ( 56.892)\tmIoU  50.208 ( 45.296)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-7/60]\tETA [00h01m31s|00h00m04s]\tTime [0.090|0.033]\tLoss 0.3687 (0.5056)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.363 ( 75.737)\tmAcc  61.218 ( 56.956)\tmIoU  51.971 ( 45.396)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-8/60]\tETA [00h01m31s|00h00m04s]\tTime [0.090|0.033]\tLoss 0.4643 (0.5031)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  76.940 ( 75.755)\tmAcc  55.785 ( 56.939)\tmIoU  45.071 ( 45.391)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-9/60]\tETA [00h01m31s|00h00m04s]\tTime [0.090|0.031]\tLoss 0.4800 (0.5003)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.201 ( 75.732)\tmAcc  57.692 ( 56.950)\tmIoU  44.952 ( 45.384)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-10/60]\tETA [00h01m30s|00h00m04s]\tTime [0.088|0.031]\tLoss 0.3649 (0.4964)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.113 ( 75.809)\tmAcc  62.468 ( 57.029)\tmIoU  52.595 ( 45.487)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-11/60]\tETA [00h01m30s|00h00m04s]\tTime [0.088|0.031]\tLoss 0.3305 (0.4916)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.470 ( 75.959)\tmAcc  66.885 ( 57.167)\tmIoU  58.970 ( 45.677)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-12/60]\tETA [00h01m29s|00h00m04s]\tTime [0.088|0.031]\tLoss 0.3956 (0.4883)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.314 ( 76.034)\tmAcc  62.280 ( 57.238)\tmIoU  50.602 ( 45.746)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-13/60]\tETA [00h01m29s|00h00m04s]\tTime [0.088|0.031]\tLoss 0.6768 (0.4894)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  64.749 ( 75.879)\tmAcc  53.496 ( 57.187)\tmIoU  39.873 ( 45.665)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-14/60]\tETA [00h01m29s|00h00m04s]\tTime [0.089|0.031]\tLoss 0.3664 (0.4861)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.954 ( 76.015)\tmAcc  63.501 ( 57.273)\tmIoU  49.912 ( 45.723)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-15/60]\tETA [00h01m29s|00h00m03s]\tTime [0.089|0.031]\tLoss 0.3409 (0.4821)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.121 ( 76.150)\tmAcc  67.063 ( 57.403)\tmIoU  56.347 ( 45.864)\n",
      "INFO - 07/15/25 15:57:07 - 0:00:12 - Epoch [1-16/60]\tETA [00h01m29s|00h00m03s]\tTime [0.089|0.031]\tLoss 0.3484 (0.4785)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.177 ( 76.256)\tmAcc  62.518 ( 57.470)\tmIoU  51.704 ( 45.941)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:12 - Epoch [1-17/60]\tETA [00h01m29s|00h00m03s]\tTime [0.089|0.031]\tLoss 0.3545 (0.4755)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.618 ( 76.403)\tmAcc  67.187 ( 57.597)\tmIoU  57.886 ( 46.096)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-18/60]\tETA [00h01m29s|00h00m03s]\tTime [0.088|0.030]\tLoss 0.4536 (0.4741)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.015 ( 76.373)\tmAcc  64.338 ( 57.683)\tmIoU  50.131 ( 46.148)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-19/60]\tETA [00h01m28s|00h00m03s]\tTime [0.088|0.030]\tLoss 0.4105 (0.4719)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.134 ( 76.408)\tmAcc  66.155 ( 57.790)\tmIoU  54.698 ( 46.256)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-20/60]\tETA [00h01m28s|00h00m03s]\tTime [0.088|0.031]\tLoss 0.3346 (0.4690)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.426 ( 76.495)\tmAcc  66.857 ( 57.904)\tmIoU  57.713 ( 46.399)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-21/60]\tETA [00h01m28s|00h00m03s]\tTime [0.088|0.030]\tLoss 0.3781 (0.4665)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.596 ( 76.558)\tmAcc  64.667 ( 57.987)\tmIoU  54.678 ( 46.502)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-22/60]\tETA [00h01m28s|00h00m03s]\tTime [0.088|0.030]\tLoss 0.3344 (0.4626)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.211 ( 76.639)\tmAcc  66.353 ( 58.089)\tmIoU  56.993 ( 46.630)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-23/60]\tETA [00h01m28s|00h00m03s]\tTime [0.088|0.030]\tLoss 0.3481 (0.4596)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.405 ( 76.721)\tmAcc  66.498 ( 58.190)\tmIoU  57.364 ( 46.759)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-24/60]\tETA [00h01m28s|00h00m03s]\tTime [0.088|0.031]\tLoss 0.3239 (0.4567)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.880 ( 76.806)\tmAcc  69.258 ( 58.322)\tmIoU  60.178 ( 46.919)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-25/60]\tETA [00h01m29s|00h00m03s]\tTime [0.089|0.032]\tLoss 0.3997 (0.4551)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  76.303 ( 76.800)\tmAcc  66.422 ( 58.417)\tmIoU  53.198 ( 46.993)\n",
      "INFO - 07/15/25 15:57:08 - 0:00:13 - Epoch [1-26/60]\tETA [00h01m29s|00h00m03s]\tTime [0.089|0.032]\tLoss 0.3629 (0.4513)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.168 ( 76.863)\tmAcc  68.596 ( 58.536)\tmIoU  58.463 ( 47.126)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:13 - Epoch [1-27/60]\tETA [00h01m29s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.4176 (0.4495)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.934 ( 76.910)\tmAcc  59.153 ( 58.543)\tmIoU  49.353 ( 47.152)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:14 - Epoch [1-28/60]\tETA [00h01m29s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.2942 (0.4456)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.011 ( 77.024)\tmAcc  71.556 ( 58.691)\tmIoU  63.208 ( 47.334)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:14 - Epoch [1-29/60]\tETA [00h01m29s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.4557 (0.4447)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  76.011 ( 77.013)\tmAcc  63.917 ( 58.749)\tmIoU  51.462 ( 47.380)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:14 - Epoch [1-30/60]\tETA [00h01m29s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.3642 (0.4420)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.649 ( 77.076)\tmAcc  65.722 ( 58.827)\tmIoU  54.752 ( 47.462)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:14 - Epoch [1-31/60]\tETA [00h01m28s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.3551 (0.4395)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.465 ( 77.146)\tmAcc  68.063 ( 58.928)\tmIoU  58.535 ( 47.584)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:14 - Epoch [1-32/60]\tETA [00h01m28s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.3282 (0.4373)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.743 ( 77.250)\tmAcc  69.174 ( 59.040)\tmIoU  57.954 ( 47.697)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:14 - Epoch [1-33/60]\tETA [00h01m28s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.3259 (0.4339)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.761 ( 77.342)\tmAcc  69.791 ( 59.155)\tmIoU  60.996 ( 47.840)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:14 - Epoch [1-34/60]\tETA [00h01m28s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.3998 (0.4324)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.929 ( 77.327)\tmAcc  68.383 ( 59.254)\tmIoU  54.637 ( 47.912)\n",
      "INFO - 07/15/25 15:57:09 - 0:00:14 - Epoch [1-35/60]\tETA [00h01m28s|00h00m02s]\tTime [0.089|0.032]\tLoss 0.3988 (0.4281)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.425 ( 77.370)\tmAcc  63.938 ( 59.303)\tmIoU  54.032 ( 47.976)\n",
      "INFO - 07/15/25 15:57:10 - 0:00:15 - Epoch [1-36/60]\tETA [00h01m29s|00h00m02s]\tTime [0.091|0.032]\tLoss 0.5516 (0.4297)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  77.571 ( 77.372)\tmAcc  52.676 ( 59.234)\tmIoU  43.594 ( 47.931)\n",
      "INFO - 07/15/25 15:57:10 - 0:00:15 - Epoch [1-37/60]\tETA [00h01m32s|00h00m02s]\tTime [0.093|0.032]\tLoss 0.3378 (0.4276)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.399 ( 77.485)\tmAcc  66.606 ( 59.310)\tmIoU  54.847 ( 48.002)\n",
      "INFO - 07/15/25 15:57:10 - 0:00:15 - Epoch [1-38/60]\tETA [00h01m35s|00h00m02s]\tTime [0.098|0.033]\tLoss 0.3370 (0.4259)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.572 ( 77.578)\tmAcc  66.016 ( 59.378)\tmIoU  57.537 ( 48.099)\n",
      "INFO - 07/15/25 15:57:10 - 0:00:15 - Epoch [1-39/60]\tETA [00h01m37s|00h00m02s]\tTime [0.100|0.033]\tLoss 0.3453 (0.4225)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.403 ( 77.647)\tmAcc  67.459 ( 59.460)\tmIoU  58.921 ( 48.209)\n",
      "INFO - 07/15/25 15:57:11 - 0:00:16 - Epoch [1-40/60]\tETA [00h01m38s|00h00m02s]\tTime [0.101|0.033]\tLoss 0.2909 (0.4191)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.825 ( 77.759)\tmAcc  68.562 ( 59.551)\tmIoU  62.058 ( 48.347)\n",
      "INFO - 07/15/25 15:57:11 - 0:00:16 - Epoch [1-41/60]\tETA [00h01m44s|00h00m02s]\tTime [0.109|0.033]\tLoss 0.3457 (0.4172)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.937 ( 78.093)\tmAcc  63.770 ( 59.683)\tmIoU  56.013 ( 48.561)\n",
      "INFO - 07/15/25 15:57:12 - 0:00:17 - Epoch [1-42/60]\tETA [00h01m50s|00h00m02s]\tTime [0.116|0.033]\tLoss 0.4258 (0.4158)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  77.474 ( 78.271)\tmAcc  59.290 ( 59.755)\tmIoU  48.028 ( 48.693)\n",
      "INFO - 07/15/25 15:57:12 - 0:00:17 - Epoch [1-43/60]\tETA [00h01m56s|00h00m02s]\tTime [0.122|0.033]\tLoss 0.3337 (0.4125)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.696 ( 78.431)\tmAcc  62.463 ( 59.848)\tmIoU  52.776 ( 48.841)\n",
      "INFO - 07/15/25 15:57:13 - 0:00:18 - Epoch [1-44/60]\tETA [00h02m02s|00h00m02s]\tTime [0.130|0.033]\tLoss 0.2787 (0.4082)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.070 ( 78.667)\tmAcc  69.009 ( 59.959)\tmIoU  62.172 ( 49.035)\n",
      "INFO - 07/15/25 15:57:13 - 0:00:18 - Epoch [1-45/60]\tETA [00h02m08s|00h00m02s]\tTime [0.137|0.033]\tLoss 0.6309 (0.4118)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  65.712 ( 78.634)\tmAcc  59.953 ( 59.977)\tmIoU  41.461 ( 49.037)\n",
      "INFO - 07/15/25 15:57:14 - 0:00:19 - Epoch [1-46/60]\tETA [00h02m14s|00h00m02s]\tTime [0.144|0.033]\tLoss 0.3180 (0.4092)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.258 ( 78.844)\tmAcc  56.543 ( 59.993)\tmIoU  48.946 ( 49.118)\n",
      "INFO - 07/15/25 15:57:14 - 0:00:19 - Epoch [1-47/60]\tETA [00h02m20s|00h00m01s]\tTime [0.151|0.033]\tLoss 0.3395 (0.4048)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.853 ( 79.025)\tmAcc  61.340 ( 60.026)\tmIoU  53.104 ( 49.212)\n",
      "INFO - 07/15/25 15:57:15 - 0:00:20 - Epoch [1-48/60]\tETA [00h02m26s|00h00m01s]\tTime [0.158|0.033]\tLoss 0.5744 (0.4076)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  67.944 ( 78.957)\tmAcc  58.956 ( 60.019)\tmIoU  41.915 ( 49.158)\n",
      "INFO - 07/15/25 15:57:15 - 0:00:20 - Epoch [1-49/60]\tETA [00h02m31s|00h00m01s]\tTime [0.165|0.034]\tLoss 0.3140 (0.4058)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.321 ( 79.100)\tmAcc  62.626 ( 60.148)\tmIoU  52.337 ( 49.302)\n",
      "INFO - 07/15/25 15:57:16 - 0:00:21 - Epoch [1-50/60]\tETA [00h02m37s|00h00m01s]\tTime [0.172|0.034]\tLoss 0.3675 (0.4013)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.090 ( 79.212)\tmAcc  66.934 ( 60.278)\tmIoU  58.824 ( 49.459)\n",
      "INFO - 07/15/25 15:57:16 - 0:00:21 - Epoch [1-51/60]\tETA [00h02m43s|00h00m01s]\tTime [0.179|0.034]\tLoss 0.3232 (0.3976)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.568 ( 79.437)\tmAcc  69.413 ( 60.405)\tmIoU  61.123 ( 49.680)\n",
      "INFO - 07/15/25 15:57:17 - 0:00:22 - Epoch [1-52/60]\tETA [00h02m49s|00h00m01s]\tTime [0.186|0.034]\tLoss 0.3724 (0.3972)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.160 ( 79.541)\tmAcc  69.027 ( 60.535)\tmIoU  61.028 ( 49.838)\n",
      "INFO - 07/15/25 15:57:17 - 0:00:22 - Epoch [1-53/60]\tETA [00h02m55s|00h00m01s]\tTime [0.193|0.034]\tLoss 0.5296 (0.3991)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  69.670 ( 79.508)\tmAcc  58.614 ( 60.567)\tmIoU  43.222 ( 49.831)\n",
      "INFO - 07/15/25 15:57:18 - 0:00:23 - Epoch [1-54/60]\tETA [00h03m00s|00h00m01s]\tTime [0.200|0.034]\tLoss 0.3825 (0.3981)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.826 ( 79.516)\tmAcc  60.619 ( 60.619)\tmIoU  52.452 ( 49.885)\n",
      "INFO - 07/15/25 15:57:18 - 0:00:23 - Epoch [1-55/60]\tETA [00h03m06s|00h00m01s]\tTime [0.207|0.034]\tLoss 0.3153 (0.3963)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.503 ( 79.582)\tmAcc  68.880 ( 60.792)\tmIoU  57.274 ( 50.007)\n",
      "INFO - 07/15/25 15:57:19 - 0:00:24 - Epoch [1-56/60]\tETA [00h03m12s|00h00m00s]\tTime [0.214|0.034]\tLoss 0.3859 (0.3957)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.234 ( 79.633)\tmAcc  63.227 ( 60.848)\tmIoU  52.296 ( 50.077)\n",
      "INFO - 07/15/25 15:57:20 - 0:00:24 - Epoch [1-57/60]\tETA [00h03m18s|00h00m00s]\tTime [0.221|0.034]\tLoss 0.3042 (0.3940)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.282 ( 79.692)\tmAcc  68.751 ( 60.977)\tmIoU  60.542 ( 50.218)\n",
      "INFO - 07/15/25 15:57:20 - 0:00:25 - Epoch [1-58/60]\tETA [00h03m24s|00h00m00s]\tTime [0.229|0.034]\tLoss 0.6212 (0.3977)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  67.253 ( 79.500)\tmAcc  60.288 ( 61.036)\tmIoU  42.968 ( 50.168)\n",
      "INFO - 07/15/25 15:57:21 - 0:00:25 - Epoch [1-59/60]\tETA [00h03m29s|00h00m00s]\tTime [0.236|0.035]\tLoss 0.4142 (0.3953)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.697 ( 79.612)\tmAcc  58.560 ( 61.052)\tmIoU  50.703 ( 50.239)\n",
      "INFO - 07/15/25 15:57:21 - 0:00:26 - Epoch [1-60/60]\tETA [00h03m35s|00h00m00s]\tTime [0.243|0.035]\tLoss 0.3157 (0.3937)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.261 ( 79.675)\tmAcc  67.339 ( 61.150)\tmIoU  59.406 ( 50.341)\n",
      "INFO - 07/15/25 15:57:21 - 0:00:26 - ============ Starting epoch 2 ... ============\n",
      "INFO - 07/15/25 15:57:22 - 0:00:27 - Epoch [2-1/60]\tETA [00h03m41s|00h00m14s]\tTime [0.250|0.035]\tLoss 0.2791 (0.3921)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.251 ( 79.684)\tmAcc  68.666 ( 61.298)\tmIoU  60.981 ( 50.479)\n",
      "INFO - 07/15/25 15:57:22 - 0:00:27 - Epoch [2-2/60]\tETA [00h03m46s|00h00m14s]\tTime [0.257|0.035]\tLoss 0.2806 (0.3897)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.411 ( 79.823)\tmAcc  70.187 ( 61.456)\tmIoU  63.770 ( 50.697)\n",
      "INFO - 07/15/25 15:57:23 - 0:00:28 - Epoch [2-3/60]\tETA [00h03m52s|00h00m15s]\tTime [0.264|0.035]\tLoss 0.2997 (0.3858)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.284 ( 79.855)\tmAcc  66.085 ( 61.577)\tmIoU  59.011 ( 50.819)\n",
      "INFO - 07/15/25 15:57:23 - 0:00:28 - Epoch [2-4/60]\tETA [00h03m58s|00h00m15s]\tTime [0.271|0.035]\tLoss 0.3889 (0.3832)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.441 ( 79.935)\tmAcc  56.545 ( 61.548)\tmIoU  48.261 ( 50.826)\n",
      "INFO - 07/15/25 15:57:24 - 0:00:29 - Epoch [2-5/60]\tETA [00h04m03s|00h00m15s]\tTime [0.278|0.035]\tLoss 0.2965 (0.3814)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.450 ( 79.967)\tmAcc  66.272 ( 61.629)\tmIoU  58.075 ( 50.904)\n",
      "INFO - 07/15/25 15:57:24 - 0:00:29 - Epoch [2-6/60]\tETA [00h04m09s|00h00m15s]\tTime [0.285|0.035]\tLoss 0.4401 (0.3819)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  76.778 ( 80.087)\tmAcc  56.241 ( 61.643)\tmIoU  45.667 ( 50.976)\n",
      "INFO - 07/15/25 15:57:25 - 0:00:30 - Epoch [2-7/60]\tETA [00h04m14s|00h00m15s]\tTime [0.292|0.035]\tLoss 0.2849 (0.3805)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.599 ( 80.154)\tmAcc  70.249 ( 61.842)\tmIoU  63.304 ( 51.173)\n",
      "INFO - 07/15/25 15:57:25 - 0:00:30 - Epoch [2-8/60]\tETA [00h04m20s|00h00m15s]\tTime [0.299|0.035]\tLoss 0.3335 (0.3784)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.014 ( 80.350)\tmAcc  59.965 ( 61.890)\tmIoU  52.150 ( 51.307)\n",
      "INFO - 07/15/25 15:57:26 - 0:00:31 - Epoch [2-9/60]\tETA [00h04m25s|00h00m15s]\tTime [0.306|0.035]\tLoss 0.4089 (0.3772)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  78.616 ( 80.349)\tmAcc  59.744 ( 61.885)\tmIoU  48.865 ( 51.300)\n",
      "INFO - 07/15/25 15:57:26 - 0:00:31 - Epoch [2-10/60]\tETA [00h04m31s|00h00m15s]\tTime [0.313|0.036]\tLoss 0.2882 (0.3759)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.991 ( 80.432)\tmAcc  64.564 ( 61.944)\tmIoU  55.144 ( 51.364)\n",
      "INFO - 07/15/25 15:57:27 - 0:00:32 - Epoch [2-11/60]\tETA [00h04m36s|00h00m15s]\tTime [0.320|0.036]\tLoss 0.3579 (0.3764)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.892 ( 80.458)\tmAcc  64.816 ( 62.016)\tmIoU  55.606 ( 51.438)\n",
      "INFO - 07/15/25 15:57:27 - 0:00:32 - Epoch [2-12/60]\tETA [00h04m42s|00h00m15s]\tTime [0.327|0.035]\tLoss 0.4135 (0.3767)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.112 ( 80.456)\tmAcc  51.047 ( 61.951)\tmIoU  42.663 ( 51.383)\n",
      "INFO - 07/15/25 15:57:28 - 0:00:33 - Epoch [2-13/60]\tETA [00h04m47s|00h00m15s]\tTime [0.334|0.035]\tLoss 0.2727 (0.3699)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.472 ( 80.680)\tmAcc  66.534 ( 62.028)\tmIoU  56.567 ( 51.529)\n",
      "INFO - 07/15/25 15:57:28 - 0:00:33 - Epoch [2-14/60]\tETA [00h04m53s|00h00m15s]\tTime [0.341|0.035]\tLoss 0.3764 (0.3701)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.254 ( 80.697)\tmAcc  63.124 ( 62.063)\tmIoU  53.186 ( 51.566)\n",
      "INFO - 07/15/25 15:57:29 - 0:00:34 - Epoch [2-15/60]\tETA [00h04m58s|00h00m15s]\tTime [0.348|0.035]\tLoss 0.3682 (0.3705)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.079 ( 80.776)\tmAcc  61.104 ( 62.184)\tmIoU  50.141 ( 51.683)\n",
      "INFO - 07/15/25 15:57:29 - 0:00:34 - Epoch [2-16/60]\tETA [00h05m03s|00h00m15s]\tTime [0.355|0.035]\tLoss 0.2661 (0.3692)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.608 ( 80.847)\tmAcc  68.426 ( 62.265)\tmIoU  61.677 ( 51.791)\n",
      "INFO - 07/15/25 15:57:30 - 0:00:35 - Epoch [2-17/60]\tETA [00h05m09s|00h00m15s]\tTime [0.362|0.035]\tLoss 0.4292 (0.3704)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  73.748 ( 80.837)\tmAcc  65.378 ( 62.329)\tmIoU  50.547 ( 51.837)\n",
      "INFO - 07/15/25 15:57:30 - 0:00:35 - Epoch [2-18/60]\tETA [00h05m14s|00h00m15s]\tTime [0.369|0.035]\tLoss 0.2839 (0.3676)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.422 ( 80.978)\tmAcc  69.250 ( 62.411)\tmIoU  59.933 ( 51.956)\n",
      "INFO - 07/15/25 15:57:31 - 0:00:36 - Epoch [2-19/60]\tETA [00h05m19s|00h00m15s]\tTime [0.375|0.035]\tLoss 0.4976 (0.3690)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.692 ( 80.977)\tmAcc  63.169 ( 62.531)\tmIoU  50.143 ( 52.049)\n",
      "INFO - 07/15/25 15:57:31 - 0:00:36 - Epoch [2-20/60]\tETA [00h05m24s|00h00m15s]\tTime [0.382|0.035]\tLoss 0.3682 (0.3696)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.753 ( 80.946)\tmAcc  68.777 ( 62.676)\tmIoU  57.949 ( 52.159)\n",
      "INFO - 07/15/25 15:57:32 - 0:00:37 - Epoch [2-21/60]\tETA [00h05m30s|00h00m15s]\tTime [0.389|0.035]\tLoss 0.6810 (0.3747)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  64.906 ( 80.823)\tmAcc  62.185 ( 62.738)\tmIoU  42.916 ( 52.132)\n",
      "INFO - 07/15/25 15:57:32 - 0:00:37 - Epoch [2-22/60]\tETA [00h05m35s|00h00m15s]\tTime [0.396|0.035]\tLoss 0.3497 (0.3749)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.049 ( 80.896)\tmAcc  66.865 ( 62.842)\tmIoU  56.589 ( 52.253)\n",
      "INFO - 07/15/25 15:57:33 - 0:00:38 - Epoch [2-23/60]\tETA [00h05m40s|00h00m14s]\tTime [0.403|0.035]\tLoss 0.2547 (0.3733)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.745 ( 81.085)\tmAcc  78.306 ( 63.069)\tmIoU  70.009 ( 52.543)\n",
      "INFO - 07/15/25 15:57:33 - 0:00:38 - Epoch [2-24/60]\tETA [00h05m46s|00h00m14s]\tTime [0.410|0.034]\tLoss 0.5179 (0.3766)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.545 ( 81.174)\tmAcc  58.010 ( 63.090)\tmIoU  46.338 ( 52.620)\n",
      "INFO - 07/15/25 15:57:34 - 0:00:39 - Epoch [2-25/60]\tETA [00h05m50s|00h00m14s]\tTime [0.416|0.034]\tLoss 0.3590 (0.3759)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.339 ( 81.138)\tmAcc  66.039 ( 63.139)\tmIoU  54.363 ( 52.631)\n",
      "INFO - 07/15/25 15:57:34 - 0:00:39 - Epoch [2-26/60]\tETA [00h05m55s|00h00m14s]\tTime [0.423|0.034]\tLoss 0.3044 (0.3749)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.877 ( 81.226)\tmAcc  72.578 ( 63.311)\tmIoU  62.799 ( 52.820)\n",
      "INFO - 07/15/25 15:57:35 - 0:00:40 - Epoch [2-27/60]\tETA [00h06m01s|00h00m14s]\tTime [0.430|0.034]\tLoss 0.3845 (0.3744)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.727 ( 81.485)\tmAcc  67.094 ( 63.420)\tmIoU  55.981 ( 53.042)\n",
      "INFO - 07/15/25 15:57:35 - 0:00:40 - Epoch [2-28/60]\tETA [00h06m06s|00h00m13s]\tTime [0.437|0.034]\tLoss 0.3133 (0.3747)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.710 ( 81.449)\tmAcc  70.883 ( 63.509)\tmIoU  60.923 ( 53.111)\n",
      "INFO - 07/15/25 15:57:36 - 0:00:41 - Epoch [2-29/60]\tETA [00h06m10s|00h00m13s]\tTime [0.443|0.033]\tLoss 0.3707 (0.3733)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.135 ( 81.473)\tmAcc  65.543 ( 63.580)\tmIoU  54.573 ( 53.179)\n",
      "INFO - 07/15/25 15:57:36 - 0:00:41 - Epoch [2-30/60]\tETA [00h06m16s|00h00m13s]\tTime [0.450|0.034]\tLoss 0.3085 (0.3723)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.489 ( 81.775)\tmAcc  72.368 ( 63.712)\tmIoU  61.509 ( 53.435)\n",
      "INFO - 07/15/25 15:57:37 - 0:00:42 - Epoch [2-31/60]\tETA [00h06m21s|00h00m13s]\tTime [0.458|0.034]\tLoss 0.3457 (0.3722)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.613 ( 81.862)\tmAcc  72.896 ( 63.856)\tmIoU  60.412 ( 53.586)\n",
      "INFO - 07/15/25 15:57:37 - 0:00:42 - Epoch [2-32/60]\tETA [00h06m27s|00h00m13s]\tTime [0.465|0.034]\tLoss 0.3176 (0.3720)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.757 ( 81.893)\tmAcc  69.067 ( 63.924)\tmIoU  57.063 ( 53.624)\n",
      "INFO - 07/15/25 15:57:38 - 0:00:43 - Epoch [2-33/60]\tETA [00h06m32s|00h00m12s]\tTime [0.473|0.034]\tLoss 0.3910 (0.3731)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  77.772 ( 81.855)\tmAcc  65.069 ( 63.988)\tmIoU  53.285 ( 53.662)\n",
      "INFO - 07/15/25 15:57:38 - 0:00:43 - Epoch [2-34/60]\tETA [00h06m37s|00h00m12s]\tTime [0.479|0.034]\tLoss 0.5218 (0.3751)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.417 ( 81.801)\tmAcc  58.451 ( 63.946)\tmIoU  46.987 ( 53.609)\n",
      "INFO - 07/15/25 15:57:39 - 0:00:44 - Epoch [2-35/60]\tETA [00h06m43s|00h00m12s]\tTime [0.487|0.034]\tLoss 0.3995 (0.3751)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  72.298 ( 81.696)\tmAcc  65.851 ( 64.016)\tmIoU  49.981 ( 53.618)\n",
      "INFO - 07/15/25 15:57:39 - 0:00:44 - Epoch [2-36/60]\tETA [00h06m47s|00h00m11s]\tTime [0.493|0.034]\tLoss 0.3235 (0.3713)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.121 ( 81.756)\tmAcc  59.260 ( 63.989)\tmIoU  49.323 ( 53.595)\n",
      "INFO - 07/15/25 15:57:40 - 0:00:45 - Epoch [2-37/60]\tETA [00h06m50s|00h00m11s]\tTime [0.497|0.033]\tLoss 0.3731 (0.3719)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.573 ( 81.733)\tmAcc  64.874 ( 64.012)\tmIoU  54.910 ( 53.605)\n",
      "INFO - 07/15/25 15:57:40 - 0:00:45 - Epoch [2-38/60]\tETA [00h06m52s|00h00m11s]\tTime [0.500|0.033]\tLoss 0.2896 (0.3711)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.206 ( 81.786)\tmAcc  70.514 ( 64.115)\tmIoU  62.151 ( 53.725)\n",
      "INFO - 07/15/25 15:57:41 - 0:00:46 - Epoch [2-39/60]\tETA [00h06m56s|00h00m10s]\tTime [0.505|0.033]\tLoss 0.2906 (0.3702)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.972 ( 81.918)\tmAcc  67.490 ( 64.296)\tmIoU  58.869 ( 53.925)\n",
      "INFO - 07/15/25 15:57:42 - 0:00:46 - Epoch [2-40/60]\tETA [00h07m00s|00h00m10s]\tTime [0.511|0.034]\tLoss 0.2945 (0.3703)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.951 ( 81.973)\tmAcc  71.751 ( 64.388)\tmIoU  64.651 ( 54.038)\n",
      "INFO - 07/15/25 15:57:42 - 0:00:47 - Epoch [2-41/60]\tETA [00h07m00s|00h00m09s]\tTime [0.511|0.034]\tLoss 0.4010 (0.3712)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.119 ( 81.895)\tmAcc  65.707 ( 64.402)\tmIoU  54.955 ( 54.028)\n",
      "INFO - 07/15/25 15:57:43 - 0:00:48 - Epoch [2-42/60]\tETA [00h06m59s|00h00m09s]\tTime [0.512|0.034]\tLoss 0.2948 (0.3690)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.512 ( 81.902)\tmAcc  68.980 ( 64.558)\tmIoU  61.471 ( 54.171)\n",
      "INFO - 07/15/25 15:57:43 - 0:00:48 - Epoch [2-43/60]\tETA [00h07m00s|00h00m08s]\tTime [0.513|0.034]\tLoss 0.3129 (0.3687)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.215 ( 81.968)\tmAcc  70.779 ( 64.745)\tmIoU  62.152 ( 54.364)\n",
      "INFO - 07/15/25 15:57:44 - 0:00:49 - Epoch [2-44/60]\tETA [00h06m59s|00h00m08s]\tTime [0.513|0.034]\tLoss 0.3034 (0.3691)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.249 ( 82.191)\tmAcc  72.413 ( 64.889)\tmIoU  64.004 ( 54.592)\n",
      "INFO - 07/15/25 15:57:44 - 0:00:49 - Epoch [2-45/60]\tETA [00h06m58s|00h00m07s]\tTime [0.512|0.034]\tLoss 0.5476 (0.3677)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  69.369 ( 82.091)\tmAcc  61.453 ( 64.924)\tmIoU  44.816 ( 54.567)\n",
      "INFO - 07/15/25 15:57:45 - 0:00:50 - Epoch [2-46/60]\tETA [00h06m58s|00h00m07s]\tTime [0.512|0.034]\tLoss 0.2466 (0.3665)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  92.851 ( 82.241)\tmAcc  70.820 ( 65.012)\tmIoU  63.467 ( 54.699)\n",
      "INFO - 07/15/25 15:57:45 - 0:00:50 - Epoch [2-47/60]\tETA [00h06m57s|00h00m06s]\tTime [0.512|0.034]\tLoss 0.2999 (0.3659)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.776 ( 82.265)\tmAcc  68.128 ( 65.082)\tmIoU  59.621 ( 54.776)\n",
      "INFO - 07/15/25 15:57:46 - 0:00:51 - Epoch [2-48/60]\tETA [00h06m57s|00h00m06s]\tTime [0.512|0.034]\tLoss 0.6354 (0.3669)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  68.705 ( 82.182)\tmAcc  63.450 ( 65.158)\tmIoU  46.005 ( 54.785)\n",
      "INFO - 07/15/25 15:57:46 - 0:00:51 - Epoch [2-49/60]\tETA [00h06m56s|00h00m05s]\tTime [0.512|0.033]\tLoss 0.2424 (0.3657)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.799 ( 82.338)\tmAcc  73.653 ( 65.318)\tmIoU  67.459 ( 55.010)\n",
      "INFO - 07/15/25 15:57:47 - 0:00:52 - Epoch [2-50/60]\tETA [00h06m56s|00h00m05s]\tTime [0.512|0.033]\tLoss 0.3512 (0.3654)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.180 ( 82.329)\tmAcc  69.412 ( 65.387)\tmIoU  58.180 ( 55.066)\n",
      "INFO - 07/15/25 15:57:47 - 0:00:52 - Epoch [2-51/60]\tETA [00h06m55s|00h00m04s]\tTime [0.513|0.033]\tLoss 0.2910 (0.3649)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.448 ( 82.309)\tmAcc  70.643 ( 65.425)\tmIoU  61.320 ( 55.089)\n",
      "INFO - 07/15/25 15:57:48 - 0:00:53 - Epoch [2-52/60]\tETA [00h06m55s|00h00m04s]\tTime [0.512|0.033]\tLoss 0.4967 (0.3669)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.988 ( 82.255)\tmAcc  64.557 ( 65.448)\tmIoU  51.966 ( 55.103)\n",
      "INFO - 07/15/25 15:57:48 - 0:00:53 - Epoch [2-53/60]\tETA [00h06m55s|00h00m03s]\tTime [0.513|0.033]\tLoss 0.3081 (0.3632)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.175 ( 82.450)\tmAcc  67.492 ( 65.588)\tmIoU  57.795 ( 55.282)\n",
      "INFO - 07/15/25 15:57:49 - 0:00:54 - Epoch [2-54/60]\tETA [00h06m54s|00h00m03s]\tTime [0.513|0.036]\tLoss 0.3459 (0.3626)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.371 ( 82.414)\tmAcc  70.995 ( 65.663)\tmIoU  60.848 ( 55.392)\n",
      "INFO - 07/15/25 15:57:49 - 0:00:54 - Epoch [2-55/60]\tETA [00h06m56s|00h00m02s]\tTime [0.516|0.036]\tLoss 0.3261 (0.3628)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.230 ( 82.375)\tmAcc  72.908 ( 65.721)\tmIoU  58.461 ( 55.413)\n",
      "INFO - 07/15/25 15:57:50 - 0:00:55 - Epoch [2-56/60]\tETA [00h06m56s|00h00m02s]\tTime [0.517|0.036]\tLoss 0.3559 (0.3623)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.231 ( 82.286)\tmAcc  68.958 ( 65.785)\tmIoU  54.460 ( 55.440)\n",
      "INFO - 07/15/25 15:57:51 - 0:00:55 - Epoch [2-57/60]\tETA [00h06m55s|00h00m01s]\tTime [0.516|0.036]\tLoss 0.4949 (0.3655)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.665 ( 82.156)\tmAcc  58.060 ( 65.694)\tmIoU  46.270 ( 55.324)\n",
      "INFO - 07/15/25 15:57:51 - 0:00:56 - Epoch [2-58/60]\tETA [00h06m55s|00h00m01s]\tTime [0.516|0.036]\tLoss 0.3278 (0.3606)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.307 ( 82.249)\tmAcc  70.842 ( 65.759)\tmIoU  59.488 ( 55.418)\n",
      "INFO - 07/15/25 15:57:52 - 0:00:57 - Epoch [2-59/60]\tETA [00h06m55s|00h00m00s]\tTime [0.517|0.036]\tLoss 0.4574 (0.3613)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.674 ( 82.284)\tmAcc  74.031 ( 65.838)\tmIoU  63.564 ( 55.506)\n",
      "INFO - 07/15/25 15:57:52 - 0:00:57 - Epoch [2-60/60]\tETA [00h06m55s|00h00m00s]\tTime [0.518|0.036]\tLoss 0.2729 (0.3606)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.522 ( 82.325)\tmAcc  76.162 ( 65.931)\tmIoU  66.067 ( 55.590)\n",
      "INFO - 07/15/25 15:57:52 - 0:00:57 - ============ Starting epoch 3 ... ============\n",
      "INFO - 07/15/25 15:57:53 - 0:00:58 - Epoch [3-1/60]\tETA [00h06m55s|00h00m30s]\tTime [0.519|0.036]\tLoss 0.4254 (0.3630)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.888 ( 82.308)\tmAcc  69.348 ( 65.978)\tmIoU  56.609 ( 55.609)\n",
      "INFO - 07/15/25 15:57:53 - 0:00:58 - Epoch [3-2/60]\tETA [00h06m55s|00h00m30s]\tTime [0.519|0.036]\tLoss 0.4485 (0.3658)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.692 ( 82.233)\tmAcc  65.666 ( 65.971)\tmIoU  52.603 ( 55.565)\n",
      "INFO - 07/15/25 15:57:54 - 0:00:59 - Epoch [3-3/60]\tETA [00h06m53s|00h00m29s]\tTime [0.518|0.036]\tLoss 0.2584 (0.3652)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.390 ( 82.273)\tmAcc  76.253 ( 66.068)\tmIoU  66.377 ( 55.655)\n",
      "INFO - 07/15/25 15:57:54 - 0:00:59 - Epoch [3-4/60]\tETA [00h06m53s|00h00m29s]\tTime [0.518|0.036]\tLoss 0.2858 (0.3634)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.877 ( 82.323)\tmAcc  73.581 ( 66.112)\tmIoU  64.173 ( 55.695)\n",
      "INFO - 07/15/25 15:57:55 - 0:01:00 - Epoch [3-5/60]\tETA [00h06m53s|00h00m28s]\tTime [0.518|0.036]\tLoss 0.3516 (0.3644)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.697 ( 82.357)\tmAcc  74.507 ( 66.193)\tmIoU  62.000 ( 55.783)\n",
      "INFO - 07/15/25 15:57:55 - 0:01:00 - Epoch [3-6/60]\tETA [00h06m52s|00h00m28s]\tTime [0.519|0.036]\tLoss 0.2610 (0.3614)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.526 ( 82.410)\tmAcc  75.813 ( 66.265)\tmIoU  67.278 ( 55.872)\n",
      "INFO - 07/15/25 15:57:56 - 0:01:01 - Epoch [3-7/60]\tETA [00h06m52s|00h00m27s]\tTime [0.519|0.036]\tLoss 0.2798 (0.3613)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.576 ( 82.477)\tmAcc  75.305 ( 66.426)\tmIoU  63.040 ( 56.008)\n",
      "INFO - 07/15/25 15:57:56 - 0:01:01 - Epoch [3-8/60]\tETA [00h06m51s|00h00m26s]\tTime [0.517|0.036]\tLoss 0.2660 (0.3602)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.604 ( 82.483)\tmAcc  76.048 ( 66.471)\tmIoU  67.357 ( 56.050)\n",
      "INFO - 07/15/25 15:57:57 - 0:01:01 - Epoch [3-9/60]\tETA [00h06m49s|00h00m26s]\tTime [0.516|0.036]\tLoss 0.3646 (0.3594)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.452 ( 82.557)\tmAcc  70.306 ( 66.535)\tmIoU  59.627 ( 56.132)\n",
      "INFO - 07/15/25 15:57:57 - 0:01:02 - Epoch [3-10/60]\tETA [00h06m48s|00h00m25s]\tTime [0.515|0.037]\tLoss 0.3402 (0.3603)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.788 ( 82.569)\tmAcc  67.267 ( 66.550)\tmIoU  57.691 ( 56.161)\n",
      "INFO - 07/15/25 15:57:57 - 0:01:02 - Epoch [3-11/60]\tETA [00h06m47s|00h00m25s]\tTime [0.515|0.037]\tLoss 0.5880 (0.3641)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  73.632 ( 82.470)\tmAcc  62.902 ( 66.499)\tmIoU  49.549 ( 56.071)\n",
      "INFO - 07/15/25 15:57:58 - 0:01:03 - Epoch [3-12/60]\tETA [00h06m46s|00h00m24s]\tTime [0.515|0.037]\tLoss 0.2809 (0.3619)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.732 ( 82.490)\tmAcc  69.111 ( 66.498)\tmIoU  60.290 ( 56.095)\n",
      "INFO - 07/15/25 15:57:58 - 0:01:03 - Epoch [3-13/60]\tETA [00h06m45s|00h00m24s]\tTime [0.513|0.037]\tLoss 0.3835 (0.3638)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.640 ( 82.449)\tmAcc  70.260 ( 66.503)\tmIoU  59.789 ( 56.082)\n",
      "INFO - 07/15/25 15:57:59 - 0:01:04 - Epoch [3-14/60]\tETA [00h06m44s|00h00m23s]\tTime [0.513|0.037]\tLoss 0.5800 (0.3672)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  73.204 ( 82.422)\tmAcc  58.360 ( 66.403)\tmIoU  45.952 ( 55.996)\n",
      "INFO - 07/15/25 15:57:59 - 0:01:04 - Epoch [3-15/60]\tETA [00h06m43s|00h00m23s]\tTime [0.513|0.037]\tLoss 0.3375 (0.3666)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.491 ( 82.412)\tmAcc  68.005 ( 66.443)\tmIoU  57.131 ( 56.027)\n",
      "INFO - 07/15/25 15:58:00 - 0:01:05 - Epoch [3-16/60]\tETA [00h06m43s|00h00m22s]\tTime [0.514|0.037]\tLoss 0.2261 (0.3660)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.432 ( 82.541)\tmAcc  75.647 ( 66.673)\tmIoU  67.739 ( 56.268)\n",
      "INFO - 07/15/25 15:58:01 - 0:01:05 - Epoch [3-17/60]\tETA [00h06m43s|00h00m22s]\tTime [0.514|0.037]\tLoss 0.2513 (0.3630)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.630 ( 82.523)\tmAcc  72.918 ( 66.736)\tmIoU  64.971 ( 56.369)\n",
      "INFO - 07/15/25 15:58:01 - 0:01:06 - Epoch [3-18/60]\tETA [00h06m43s|00h00m21s]\tTime [0.514|0.037]\tLoss 0.3184 (0.3636)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.184 ( 82.489)\tmAcc  69.588 ( 66.772)\tmIoU  60.147 ( 56.395)\n",
      "INFO - 07/15/25 15:58:02 - 0:01:06 - Epoch [3-19/60]\tETA [00h06m43s|00h00m21s]\tTime [0.515|0.037]\tLoss 0.2850 (0.3600)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.725 ( 82.503)\tmAcc  68.151 ( 66.779)\tmIoU  60.176 ( 56.408)\n",
      "INFO - 07/15/25 15:58:02 - 0:01:07 - Epoch [3-20/60]\tETA [00h06m42s|00h00m20s]\tTime [0.515|0.037]\tLoss 0.4847 (0.3620)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.515 ( 82.369)\tmAcc  65.580 ( 66.749)\tmIoU  52.132 ( 56.309)\n",
      "INFO - 07/15/25 15:58:03 - 0:01:07 - Epoch [3-21/60]\tETA [00h06m41s|00h00m20s]\tTime [0.514|0.037]\tLoss 0.3693 (0.3568)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.862 ( 82.309)\tmAcc  68.740 ( 66.799)\tmIoU  57.971 ( 56.328)\n",
      "INFO - 07/15/25 15:58:03 - 0:01:08 - Epoch [3-22/60]\tETA [00h06m41s|00h00m19s]\tTime [0.515|0.037]\tLoss 0.2922 (0.3558)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.425 ( 82.408)\tmAcc  71.791 ( 66.924)\tmIoU  58.875 ( 56.437)\n",
      "INFO - 07/15/25 15:58:04 - 0:01:08 - Epoch [3-23/60]\tETA [00h06m40s|00h00m19s]\tTime [0.514|0.037]\tLoss 0.2288 (0.3554)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.338 ( 82.495)\tmAcc  75.348 ( 67.053)\tmIoU  68.628 ( 56.595)\n",
      "INFO - 07/15/25 15:58:04 - 0:01:09 - Epoch [3-24/60]\tETA [00h06m40s|00h00m18s]\tTime [0.514|0.037]\tLoss 0.2825 (0.3515)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.213 ( 82.486)\tmAcc  78.619 ( 67.149)\tmIoU  69.583 ( 56.669)\n",
      "INFO - 07/15/25 15:58:05 - 0:01:09 - Epoch [3-25/60]\tETA [00h06m40s|00h00m18s]\tTime [0.515|0.037]\tLoss 0.3855 (0.3519)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.054 ( 82.619)\tmAcc  65.304 ( 67.202)\tmIoU  52.987 ( 56.785)\n",
      "INFO - 07/15/25 15:58:05 - 0:01:10 - Epoch [3-26/60]\tETA [00h06m39s|00h00m17s]\tTime [0.514|0.037]\tLoss 0.3410 (0.3525)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.982 ( 82.577)\tmAcc  71.546 ( 67.352)\tmIoU  61.124 ( 56.906)\n",
      "INFO - 07/15/25 15:58:06 - 0:01:11 - Epoch [3-27/60]\tETA [00h06m38s|00h00m16s]\tTime [0.514|0.037]\tLoss 0.3936 (0.3527)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.593 ( 82.534)\tmAcc  67.721 ( 67.416)\tmIoU  56.721 ( 56.943)\n",
      "INFO - 07/15/25 15:58:06 - 0:01:11 - Epoch [3-28/60]\tETA [00h06m38s|00h00m16s]\tTime [0.515|0.038]\tLoss 0.4075 (0.3543)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.733 ( 82.652)\tmAcc  60.298 ( 67.429)\tmIoU  49.936 ( 57.023)\n",
      "INFO - 07/15/25 15:58:07 - 0:01:12 - Epoch [3-29/60]\tETA [00h06m38s|00h00m15s]\tTime [0.516|0.038]\tLoss 0.4088 (0.3549)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.176 ( 82.651)\tmAcc  64.062 ( 67.444)\tmIoU  50.362 ( 57.003)\n",
      "INFO - 07/15/25 15:58:07 - 0:01:12 - Epoch [3-30/60]\tETA [00h06m38s|00h00m15s]\tTime [0.516|0.038]\tLoss 0.3080 (0.3549)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.961 ( 82.619)\tmAcc  75.219 ( 67.527)\tmIoU  63.507 ( 57.050)\n",
      "INFO - 07/15/25 15:58:08 - 0:01:13 - Epoch [3-31/60]\tETA [00h06m38s|00h00m14s]\tTime [0.516|0.038]\tLoss 0.3773 (0.3554)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  78.840 ( 82.552)\tmAcc  68.489 ( 67.517)\tmIoU  47.524 ( 56.914)\n",
      "INFO - 07/15/25 15:58:08 - 0:01:13 - Epoch [3-32/60]\tETA [00h06m37s|00h00m14s]\tTime [0.516|0.038]\tLoss 0.2876 (0.3549)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.648 ( 82.567)\tmAcc  75.414 ( 67.581)\tmIoU  65.321 ( 56.957)\n",
      "INFO - 07/15/25 15:58:09 - 0:01:14 - Epoch [3-33/60]\tETA [00h06m37s|00h00m13s]\tTime [0.516|0.037]\tLoss 0.3320 (0.3539)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.130 ( 82.681)\tmAcc  74.348 ( 67.739)\tmIoU  62.919 ( 57.154)\n",
      "INFO - 07/15/25 15:58:09 - 0:01:14 - Epoch [3-34/60]\tETA [00h06m36s|00h00m13s]\tTime [0.516|0.037]\tLoss 0.2973 (0.3502)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.740 ( 82.661)\tmAcc  77.413 ( 67.907)\tmIoU  67.287 ( 57.302)\n",
      "INFO - 07/15/25 15:58:10 - 0:01:15 - Epoch [3-35/60]\tETA [00h06m35s|00h00m12s]\tTime [0.515|0.037]\tLoss 0.3399 (0.3492)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.080 ( 82.586)\tmAcc  72.682 ( 67.945)\tmIoU  62.146 ( 57.351)\n",
      "INFO - 07/15/25 15:58:10 - 0:01:15 - Epoch [3-36/60]\tETA [00h06m34s|00h00m12s]\tTime [0.515|0.037]\tLoss 0.4961 (0.3521)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.886 ( 82.543)\tmAcc  64.634 ( 67.959)\tmIoU  51.288 ( 57.341)\n",
      "INFO - 07/15/25 15:58:11 - 0:01:16 - Epoch [3-37/60]\tETA [00h06m34s|00h00m11s]\tTime [0.515|0.037]\tLoss 0.3167 (0.3511)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.215 ( 82.542)\tmAcc  69.851 ( 67.970)\tmIoU  58.583 ( 57.321)\n",
      "INFO - 07/15/25 15:58:11 - 0:01:16 - Epoch [3-38/60]\tETA [00h06m33s|00h00m11s]\tTime [0.514|0.038]\tLoss 0.2812 (0.3510)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.227 ( 82.762)\tmAcc  75.687 ( 68.124)\tmIoU  65.568 ( 57.547)\n",
      "INFO - 07/15/25 15:58:12 - 0:01:17 - Epoch [3-39/60]\tETA [00h06m32s|00h00m10s]\tTime [0.515|0.037]\tLoss 0.2752 (0.3507)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.000 ( 82.795)\tmAcc  74.458 ( 68.283)\tmIoU  63.254 ( 57.673)\n",
      "INFO - 07/15/25 15:58:12 - 0:01:17 - Epoch [3-40/60]\tETA [00h06m32s|00h00m10s]\tTime [0.514|0.037]\tLoss 0.4911 (0.3540)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  76.203 ( 82.664)\tmAcc  67.970 ( 68.289)\tmIoU  54.619 ( 57.625)\n",
      "INFO - 07/15/25 15:58:13 - 0:01:18 - Epoch [3-41/60]\tETA [00h06m31s|00h00m09s]\tTime [0.514|0.037]\tLoss 0.3027 (0.3524)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.112 ( 82.643)\tmAcc  75.298 ( 68.355)\tmIoU  66.413 ( 57.679)\n",
      "INFO - 07/15/25 15:58:13 - 0:01:18 - Epoch [3-42/60]\tETA [00h06m30s|00h00m09s]\tTime [0.513|0.037]\tLoss 0.2849 (0.3522)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.248 ( 82.641)\tmAcc  73.452 ( 68.388)\tmIoU  63.398 ( 57.675)\n",
      "INFO - 07/15/25 15:58:14 - 0:01:19 - Epoch [3-43/60]\tETA [00h06m29s|00h00m08s]\tTime [0.512|0.037]\tLoss 0.2845 (0.3517)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.776 ( 82.656)\tmAcc  74.933 ( 68.476)\tmIoU  62.767 ( 57.713)\n",
      "INFO - 07/15/25 15:58:14 - 0:01:19 - Epoch [3-44/60]\tETA [00h06m28s|00h00m08s]\tTime [0.512|0.037]\tLoss 0.3046 (0.3517)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.786 ( 82.670)\tmAcc  72.459 ( 68.636)\tmIoU  62.191 ( 57.852)\n",
      "INFO - 07/15/25 15:58:15 - 0:01:20 - Epoch [3-45/60]\tETA [00h06m28s|00h00m07s]\tTime [0.513|0.037]\tLoss 0.3099 (0.3478)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.390 ( 82.649)\tmAcc  72.550 ( 68.698)\tmIoU  62.531 ( 57.897)\n",
      "INFO - 07/15/25 15:58:15 - 0:01:20 - Epoch [3-46/60]\tETA [00h06m28s|00h00m07s]\tTime [0.513|0.037]\tLoss 0.3208 (0.3490)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.548 ( 82.727)\tmAcc  68.077 ( 68.817)\tmIoU  56.960 ( 58.010)\n",
      "INFO - 07/15/25 15:58:16 - 0:01:21 - Epoch [3-47/60]\tETA [00h06m27s|00h00m06s]\tTime [0.513|0.037]\tLoss 0.3325 (0.3496)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.954 ( 82.660)\tmAcc  68.331 ( 68.797)\tmIoU  58.276 ( 57.959)\n",
      "INFO - 07/15/25 15:58:16 - 0:01:21 - Epoch [3-48/60]\tETA [00h06m27s|00h00m06s]\tTime [0.513|0.037]\tLoss 0.2880 (0.3438)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.116 ( 82.651)\tmAcc  68.985 ( 68.888)\tmIoU  59.940 ( 58.037)\n",
      "INFO - 07/15/25 15:58:17 - 0:01:22 - Epoch [3-49/60]\tETA [00h06m26s|00h00m05s]\tTime [0.514|0.037]\tLoss 0.2893 (0.3446)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.495 ( 82.730)\tmAcc  70.260 ( 68.993)\tmIoU  62.626 ( 58.175)\n",
      "INFO - 07/15/25 15:58:17 - 0:01:22 - Epoch [3-50/60]\tETA [00h06m26s|00h00m05s]\tTime [0.514|0.037]\tLoss 0.3501 (0.3445)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.567 ( 82.696)\tmAcc  69.977 ( 69.047)\tmIoU  61.327 ( 58.237)\n",
      "INFO - 07/15/25 15:58:18 - 0:01:23 - Epoch [3-51/60]\tETA [00h06m25s|00h00m04s]\tTime [0.513|0.037]\tLoss 0.3496 (0.3455)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.529 ( 82.712)\tmAcc  62.715 ( 69.026)\tmIoU  53.349 ( 58.214)\n",
      "INFO - 07/15/25 15:58:19 - 0:01:23 - Epoch [3-52/60]\tETA [00h06m25s|00h00m04s]\tTime [0.513|0.037]\tLoss 0.3094 (0.3424)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.548 ( 82.757)\tmAcc  72.134 ( 69.237)\tmIoU  63.144 ( 58.419)\n",
      "INFO - 07/15/25 15:58:19 - 0:01:24 - Epoch [3-53/60]\tETA [00h06m24s|00h00m03s]\tTime [0.513|0.037]\tLoss 0.4086 (0.3441)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  78.278 ( 82.645)\tmAcc  67.887 ( 69.250)\tmIoU  55.725 ( 58.411)\n",
      "INFO - 07/15/25 15:58:20 - 0:01:24 - Epoch [3-54/60]\tETA [00h06m24s|00h00m03s]\tTime [0.514|0.034]\tLoss 0.3832 (0.3447)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.615 ( 82.648)\tmAcc  66.805 ( 69.287)\tmIoU  56.712 ( 58.446)\n",
      "INFO - 07/15/25 15:58:20 - 0:01:25 - Epoch [3-55/60]\tETA [00h06m21s|00h00m02s]\tTime [0.511|0.034]\tLoss 0.2961 (0.3442)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.781 ( 82.695)\tmAcc  75.264 ( 69.429)\tmIoU  65.481 ( 58.599)\n",
      "INFO - 07/15/25 15:58:21 - 0:01:25 - Epoch [3-56/60]\tETA [00h06m20s|00h00m02s]\tTime [0.510|0.034]\tLoss 0.3686 (0.3444)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.789 ( 82.647)\tmAcc  72.476 ( 69.469)\tmIoU  62.774 ( 58.610)\n",
      "INFO - 07/15/25 15:58:21 - 0:01:26 - Epoch [3-57/60]\tETA [00h06m19s|00h00m01s]\tTime [0.510|0.034]\tLoss 0.3945 (0.3427)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.316 ( 82.753)\tmAcc  64.179 ( 69.457)\tmIoU  50.480 ( 58.610)\n",
      "INFO - 07/15/25 15:58:22 - 0:01:27 - Epoch [3-58/60]\tETA [00h06m19s|00h00m01s]\tTime [0.509|0.034]\tLoss 0.2525 (0.3415)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.407 ( 82.743)\tmAcc  77.669 ( 69.541)\tmIoU  69.304 ( 58.703)\n",
      "INFO - 07/15/25 15:58:22 - 0:01:27 - Epoch [3-59/60]\tETA [00h06m18s|00h00m00s]\tTime [0.509|0.035]\tLoss 0.2439 (0.3379)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.770 ( 82.884)\tmAcc  76.878 ( 69.679)\tmIoU  64.831 ( 58.850)\n",
      "INFO - 07/15/25 15:58:23 - 0:01:28 - Epoch [3-60/60]\tETA [00h06m16s|00h00m00s]\tTime [0.507|0.035]\tLoss 0.2385 (0.3373)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.554 ( 82.962)\tmAcc  80.921 ( 69.800)\tmIoU  70.173 ( 58.972)\n",
      "INFO - 07/15/25 15:58:23 - 0:01:28 - ============ Starting epoch 4 ... ============\n",
      "INFO - 07/15/25 15:58:23 - 0:01:28 - Epoch [4-1/60]\tETA [00h06m16s|00h00m29s]\tTime [0.507|0.035]\tLoss 0.4617 (0.3380)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.034 ( 83.103)\tmAcc  65.665 ( 69.835)\tmIoU  54.038 ( 59.084)\n",
      "INFO - 07/15/25 15:58:24 - 0:01:29 - Epoch [4-2/60]\tETA [00h06m16s|00h00m29s]\tTime [0.508|0.035]\tLoss 0.4642 (0.3382)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.585 ( 83.068)\tmAcc  73.905 ( 69.905)\tmIoU  61.470 ( 59.132)\n",
      "INFO - 07/15/25 15:58:24 - 0:01:29 - Epoch [4-3/60]\tETA [00h06m16s|00h00m28s]\tTime [0.509|0.035]\tLoss 0.2702 (0.3384)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.752 ( 83.038)\tmAcc  76.020 ( 69.882)\tmIoU  61.577 ( 59.048)\n",
      "INFO - 07/15/25 15:58:25 - 0:01:30 - Epoch [4-4/60]\tETA [00h06m15s|00h00m28s]\tTime [0.508|0.035]\tLoss 0.2633 (0.3380)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.502 ( 83.168)\tmAcc  79.970 ( 70.102)\tmIoU  70.664 ( 59.291)\n",
      "INFO - 07/15/25 15:58:25 - 0:01:30 - Epoch [4-5/60]\tETA [00h06m15s|00h00m27s]\tTime [0.509|0.035]\tLoss 0.3114 (0.3374)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.363 ( 83.198)\tmAcc  73.049 ( 70.172)\tmIoU  62.185 ( 59.370)\n",
      "INFO - 07/15/25 15:58:26 - 0:01:31 - Epoch [4-6/60]\tETA [00h06m15s|00h00m27s]\tTime [0.509|0.035]\tLoss 0.5179 (0.3416)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.590 ( 83.105)\tmAcc  65.812 ( 70.104)\tmIoU  52.881 ( 59.270)\n",
      "INFO - 07/15/25 15:58:26 - 0:01:31 - Epoch [4-7/60]\tETA [00h06m13s|00h00m26s]\tTime [0.508|0.035]\tLoss 0.2810 (0.3417)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.721 ( 83.155)\tmAcc  76.742 ( 70.201)\tmIoU  66.452 ( 59.375)\n",
      "INFO - 07/15/25 15:58:27 - 0:01:32 - Epoch [4-8/60]\tETA [00h06m13s|00h00m26s]\tTime [0.509|0.035]\tLoss 0.2495 (0.3414)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.526 ( 83.203)\tmAcc  81.083 ( 70.303)\tmIoU  72.166 ( 59.488)\n",
      "INFO - 07/15/25 15:58:27 - 0:01:32 - Epoch [4-9/60]\tETA [00h06m14s|00h00m26s]\tTime [0.510|0.035]\tLoss 0.2692 (0.3398)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.231 ( 83.284)\tmAcc  77.014 ( 70.418)\tmIoU  67.864 ( 59.620)\n",
      "INFO - 07/15/25 15:58:28 - 0:01:33 - Epoch [4-10/60]\tETA [00h06m14s|00h00m25s]\tTime [0.511|0.035]\tLoss 0.4754 (0.3421)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.337 ( 83.213)\tmAcc  72.019 ( 70.414)\tmIoU  60.502 ( 59.610)\n",
      "INFO - 07/15/25 15:58:28 - 0:01:33 - Epoch [4-11/60]\tETA [00h06m13s|00h00m25s]\tTime [0.511|0.035]\tLoss 0.3223 (0.3376)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.061 ( 83.227)\tmAcc  70.676 ( 70.392)\tmIoU  58.929 ( 59.596)\n",
      "INFO - 07/15/25 15:58:29 - 0:01:33 - Epoch [4-12/60]\tETA [00h06m12s|00h00m24s]\tTime [0.509|0.035]\tLoss 0.2787 (0.3376)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.226 ( 83.242)\tmAcc  77.420 ( 70.475)\tmIoU  62.648 ( 59.651)\n",
      "INFO - 07/15/25 15:58:29 - 0:01:34 - Epoch [4-13/60]\tETA [00h06m12s|00h00m23s]\tTime [0.510|0.034]\tLoss 0.3093 (0.3363)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.941 ( 83.344)\tmAcc  72.204 ( 70.547)\tmIoU  64.290 ( 59.761)\n",
      "INFO - 07/15/25 15:58:30 - 0:01:34 - Epoch [4-14/60]\tETA [00h06m11s|00h00m23s]\tTime [0.510|0.034]\tLoss 0.3526 (0.3326)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.967 ( 83.399)\tmAcc  72.839 ( 70.691)\tmIoU  61.600 ( 59.908)\n",
      "INFO - 07/15/25 15:58:30 - 0:01:35 - Epoch [4-15/60]\tETA [00h06m11s|00h00m22s]\tTime [0.510|0.034]\tLoss 0.2968 (0.3319)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.249 ( 83.519)\tmAcc  72.705 ( 70.759)\tmIoU  61.110 ( 60.019)\n",
      "INFO - 07/15/25 15:58:31 - 0:01:36 - Epoch [4-16/60]\tETA [00h06m11s|00h00m22s]\tTime [0.511|0.035]\tLoss 0.2078 (0.3316)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.595 ( 83.573)\tmAcc  81.792 ( 70.984)\tmIoU  75.771 ( 60.283)\n",
      "INFO - 07/15/25 15:58:31 - 0:01:36 - Epoch [4-17/60]\tETA [00h06m09s|00h00m21s]\tTime [0.510|0.035]\tLoss 0.5881 (0.3372)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.360 ( 83.501)\tmAcc  60.983 ( 70.946)\tmIoU  48.774 ( 60.222)\n",
      "INFO - 07/15/25 15:58:32 - 0:01:36 - Epoch [4-18/60]\tETA [00h06m09s|00h00m21s]\tTime [0.510|0.035]\tLoss 0.2135 (0.3354)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.431 ( 83.543)\tmAcc  79.665 ( 71.037)\tmIoU  72.826 ( 60.329)\n",
      "INFO - 07/15/25 15:58:32 - 0:01:37 - Epoch [4-19/60]\tETA [00h06m07s|00h00m20s]\tTime [0.508|0.035]\tLoss 0.3410 (0.3364)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.700 ( 83.501)\tmAcc  71.290 ( 71.075)\tmIoU  54.623 ( 60.286)\n",
      "INFO - 07/15/25 15:58:32 - 0:01:37 - Epoch [4-20/60]\tETA [00h06m06s|00h00m20s]\tTime [0.507|0.035]\tLoss 0.2871 (0.3331)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.247 ( 83.484)\tmAcc  74.487 ( 71.102)\tmIoU  65.434 ( 60.294)\n",
      "INFO - 07/15/25 15:58:33 - 0:01:38 - Epoch [4-21/60]\tETA [00h06m05s|00h00m19s]\tTime [0.506|0.034]\tLoss 0.3278 (0.3324)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.066 ( 83.523)\tmAcc  71.085 ( 71.156)\tmIoU  61.471 ( 60.359)\n",
      "INFO - 07/15/25 15:58:33 - 0:01:38 - Epoch [4-22/60]\tETA [00h06m03s|00h00m19s]\tTime [0.504|0.034]\tLoss 0.2291 (0.3313)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.768 ( 83.556)\tmAcc  81.206 ( 71.278)\tmIoU  72.418 ( 60.469)\n",
      "INFO - 07/15/25 15:58:34 - 0:01:39 - Epoch [4-23/60]\tETA [00h06m02s|00h00m18s]\tTime [0.504|0.034]\tLoss 0.2869 (0.3323)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.429 ( 83.578)\tmAcc  72.936 ( 71.300)\tmIoU  63.958 ( 60.487)\n",
      "INFO - 07/15/25 15:58:34 - 0:01:39 - Epoch [4-24/60]\tETA [00h06m02s|00h00m18s]\tTime [0.504|0.034]\tLoss 0.4341 (0.3348)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  77.004 ( 83.445)\tmAcc  71.761 ( 71.294)\tmIoU  57.828 ( 60.425)\n",
      "INFO - 07/15/25 15:58:35 - 0:01:40 - Epoch [4-25/60]\tETA [00h06m02s|00h00m17s]\tTime [0.505|0.034]\tLoss 0.2949 (0.3333)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.451 ( 83.616)\tmAcc  71.405 ( 71.393)\tmIoU  62.000 ( 60.597)\n",
      "INFO - 07/15/25 15:58:35 - 0:01:40 - Epoch [4-26/60]\tETA [00h06m01s|00h00m17s]\tTime [0.505|0.034]\tLoss 0.3838 (0.3340)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.341 ( 83.521)\tmAcc  63.374 ( 71.319)\tmIoU  48.817 ( 60.450)\n",
      "INFO - 07/15/25 15:58:36 - 0:01:41 - Epoch [4-27/60]\tETA [00h06m00s|00h00m16s]\tTime [0.504|0.034]\tLoss 0.2950 (0.3324)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.607 ( 83.519)\tmAcc  75.908 ( 71.396)\tmIoU  66.672 ( 60.521)\n",
      "INFO - 07/15/25 15:58:36 - 0:01:41 - Epoch [4-28/60]\tETA [00h06m00s|00h00m16s]\tTime [0.504|0.034]\tLoss 0.4124 (0.3325)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.260 ( 83.675)\tmAcc  56.922 ( 71.331)\tmIoU  45.965 ( 60.520)\n",
      "INFO - 07/15/25 15:58:37 - 0:01:42 - Epoch [4-29/60]\tETA [00h05m59s|00h00m15s]\tTime [0.504|0.034]\tLoss 0.4592 (0.3333)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  73.637 ( 83.513)\tmAcc  67.657 ( 71.271)\tmIoU  52.344 ( 60.369)\n",
      "INFO - 07/15/25 15:58:37 - 0:01:42 - Epoch [4-30/60]\tETA [00h05m58s|00h00m15s]\tTime [0.503|0.034]\tLoss 0.3243 (0.3336)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.536 ( 83.567)\tmAcc  73.228 ( 71.309)\tmIoU  64.854 ( 60.436)\n",
      "INFO - 07/15/25 15:58:38 - 0:01:43 - Epoch [4-31/60]\tETA [00h05m58s|00h00m14s]\tTime [0.504|0.034]\tLoss 0.3021 (0.3323)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.550 ( 83.568)\tmAcc  72.519 ( 71.328)\tmIoU  63.402 ( 60.457)\n",
      "INFO - 07/15/25 15:58:39 - 0:01:43 - Epoch [4-32/60]\tETA [00h05m58s|00h00m14s]\tTime [0.504|0.034]\tLoss 0.2551 (0.3318)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.640 ( 83.704)\tmAcc  75.002 ( 71.433)\tmIoU  67.774 ( 60.615)\n",
      "INFO - 07/15/25 15:58:39 - 0:01:44 - Epoch [4-33/60]\tETA [00h05m57s|00h00m13s]\tTime [0.504|0.034]\tLoss 0.2708 (0.3308)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.576 ( 83.738)\tmAcc  73.694 ( 71.495)\tmIoU  66.210 ( 60.699)\n",
      "INFO - 07/15/25 15:58:40 - 0:01:44 - Epoch [4-34/60]\tETA [00h05m56s|00h00m13s]\tTime [0.504|0.034]\tLoss 0.2539 (0.3300)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.191 ( 83.827)\tmAcc  80.615 ( 71.591)\tmIoU  69.911 ( 60.790)\n",
      "INFO - 07/15/25 15:58:40 - 0:01:45 - Epoch [4-35/60]\tETA [00h05m56s|00h00m12s]\tTime [0.503|0.034]\tLoss 0.2022 (0.3277)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  93.581 ( 83.940)\tmAcc  82.780 ( 71.689)\tmIoU  75.470 ( 60.960)\n",
      "INFO - 07/15/25 15:58:41 - 0:01:45 - Epoch [4-36/60]\tETA [00h05m55s|00h00m12s]\tTime [0.504|0.034]\tLoss 0.2281 (0.3233)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.536 ( 84.103)\tmAcc  80.000 ( 71.800)\tmIoU  74.174 ( 61.157)\n",
      "INFO - 07/15/25 15:58:41 - 0:01:46 - Epoch [4-37/60]\tETA [00h05m55s|00h00m11s]\tTime [0.504|0.033]\tLoss 0.2670 (0.3225)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.510 ( 84.272)\tmAcc  72.122 ( 71.941)\tmIoU  61.136 ( 61.306)\n",
      "INFO - 07/15/25 15:58:41 - 0:01:46 - Epoch [4-38/60]\tETA [00h05m54s|00h00m11s]\tTime [0.503|0.033]\tLoss 0.2299 (0.3216)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.707 ( 84.346)\tmAcc  79.155 ( 72.024)\tmIoU  73.139 ( 61.442)\n",
      "INFO - 07/15/25 15:58:42 - 0:01:47 - Epoch [4-39/60]\tETA [00h05m53s|00h00m10s]\tTime [0.502|0.033]\tLoss 0.4419 (0.3244)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  77.903 ( 84.298)\tmAcc  71.667 ( 72.000)\tmIoU  58.582 ( 61.392)\n",
      "INFO - 07/15/25 15:58:42 - 0:01:47 - Epoch [4-40/60]\tETA [00h05m52s|00h00m10s]\tTime [0.502|0.033]\tLoss 0.2042 (0.3196)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.586 ( 84.339)\tmAcc  82.032 ( 72.059)\tmIoU  74.907 ( 61.481)\n",
      "INFO - 07/15/25 15:58:43 - 0:01:48 - Epoch [4-41/60]\tETA [00h05m52s|00h00m09s]\tTime [0.502|0.033]\tLoss 0.2047 (0.3180)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.272 ( 84.452)\tmAcc  82.729 ( 72.192)\tmIoU  74.613 ( 61.661)\n",
      "INFO - 07/15/25 15:58:43 - 0:01:48 - Epoch [4-42/60]\tETA [00h05m51s|00h00m09s]\tTime [0.502|0.033]\tLoss 0.2945 (0.3181)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.435 ( 84.560)\tmAcc  73.145 ( 72.267)\tmIoU  63.610 ( 61.771)\n",
      "INFO - 07/15/25 15:58:44 - 0:01:49 - Epoch [4-43/60]\tETA [00h05m51s|00h00m08s]\tTime [0.502|0.033]\tLoss 0.2625 (0.3178)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.992 ( 84.586)\tmAcc  73.795 ( 72.243)\tmIoU  64.102 ( 61.748)\n",
      "INFO - 07/15/25 15:58:45 - 0:01:49 - Epoch [4-44/60]\tETA [00h05m51s|00h00m08s]\tTime [0.502|0.033]\tLoss 0.3233 (0.3181)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.104 ( 84.558)\tmAcc  75.416 ( 72.261)\tmIoU  66.257 ( 61.769)\n",
      "INFO - 07/15/25 15:58:45 - 0:01:50 - Epoch [4-45/60]\tETA [00h05m50s|00h00m07s]\tTime [0.503|0.033]\tLoss 0.2749 (0.3175)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.016 ( 84.631)\tmAcc  77.149 ( 72.287)\tmIoU  65.853 ( 61.807)\n",
      "INFO - 07/15/25 15:58:46 - 0:01:51 - Epoch [4-46/60]\tETA [00h05m50s|00h00m07s]\tTime [0.503|0.033]\tLoss 0.2315 (0.3160)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.582 ( 84.662)\tmAcc  78.473 ( 72.314)\tmIoU  67.780 ( 61.812)\n",
      "INFO - 07/15/25 15:58:46 - 0:01:51 - Epoch [4-47/60]\tETA [00h05m50s|00h00m06s]\tTime [0.504|0.033]\tLoss 0.5140 (0.3190)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.355 ( 84.530)\tmAcc  70.212 ( 72.263)\tmIoU  55.061 ( 61.733)\n",
      "INFO - 07/15/25 15:58:47 - 0:01:51 - Epoch [4-48/60]\tETA [00h05m49s|00h00m06s]\tTime [0.502|0.033]\tLoss 0.1888 (0.3174)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  93.167 ( 84.585)\tmAcc  83.150 ( 72.334)\tmIoU  78.031 ( 61.839)\n",
      "INFO - 07/15/25 15:58:47 - 0:01:52 - Epoch [4-49/60]\tETA [00h05m47s|00h00m05s]\tTime [0.501|0.033]\tLoss 0.2193 (0.3162)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.508 ( 84.656)\tmAcc  79.899 ( 72.430)\tmIoU  70.750 ( 61.951)\n",
      "INFO - 07/15/25 15:58:48 - 0:01:52 - Epoch [4-50/60]\tETA [00h05m46s|00h00m04s]\tTime [0.500|0.033]\tLoss 0.2563 (0.3146)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.911 ( 84.697)\tmAcc  77.205 ( 72.529)\tmIoU  69.552 ( 62.069)\n",
      "INFO - 07/15/25 15:58:48 - 0:01:53 - Epoch [4-51/60]\tETA [00h05m46s|00h00m04s]\tTime [0.501|0.033]\tLoss 0.2441 (0.3129)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.673 ( 84.838)\tmAcc  79.797 ( 72.698)\tmIoU  70.766 ( 62.281)\n",
      "INFO - 07/15/25 15:58:49 - 0:01:53 - Epoch [4-52/60]\tETA [00h05m46s|00h00m04s]\tTime [0.501|0.033]\tLoss 0.2865 (0.3125)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.965 ( 84.830)\tmAcc  77.483 ( 72.782)\tmIoU  68.439 ( 62.363)\n",
      "INFO - 07/15/25 15:58:49 - 0:01:54 - Epoch [4-53/60]\tETA [00h05m45s|00h00m03s]\tTime [0.501|0.033]\tLoss 0.3724 (0.3119)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.807 ( 84.832)\tmAcc  75.264 ( 72.832)\tmIoU  64.149 ( 62.407)\n",
      "INFO - 07/15/25 15:58:49 - 0:01:54 - Epoch [4-54/60]\tETA [00h05m43s|00h00m02s]\tTime [0.499|0.033]\tLoss 0.3276 (0.3110)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.880 ( 84.938)\tmAcc  71.715 ( 72.966)\tmIoU  58.439 ( 62.531)\n",
      "INFO - 07/15/25 15:58:50 - 0:01:55 - Epoch [4-55/60]\tETA [00h05m42s|00h00m02s]\tTime [0.497|0.033]\tLoss 0.4045 (0.3128)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.125 ( 84.945)\tmAcc  72.280 ( 73.008)\tmIoU  61.221 ( 62.572)\n",
      "INFO - 07/15/25 15:58:50 - 0:01:55 - Epoch [4-56/60]\tETA [00h05m40s|00h00m01s]\tTime [0.496|0.033]\tLoss 0.3572 (0.3126)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.854 ( 84.909)\tmAcc  72.241 ( 72.974)\tmIoU  63.997 ( 62.535)\n",
      "INFO - 07/15/25 15:58:51 - 0:01:56 - Epoch [4-57/60]\tETA [00h05m39s|00h00m01s]\tTime [0.494|0.033]\tLoss 0.2322 (0.3099)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.063 ( 84.943)\tmAcc  81.947 ( 73.065)\tmIoU  70.549 ( 62.591)\n",
      "INFO - 07/15/25 15:58:51 - 0:01:56 - Epoch [4-58/60]\tETA [00h05m37s|00h00m00s]\tTime [0.492|0.032]\tLoss 0.2916 (0.3105)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.699 ( 84.968)\tmAcc  78.133 ( 73.150)\tmIoU  68.869 ( 62.678)\n",
      "INFO - 07/15/25 15:58:52 - 0:01:57 - Epoch [4-59/60]\tETA [00h05m37s|00h00m00s]\tTime [0.493|0.032]\tLoss 0.3621 (0.3125)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.545 ( 84.937)\tmAcc  73.380 ( 73.202)\tmIoU  62.625 ( 62.702)\n",
      "INFO - 07/15/25 15:58:52 - 0:01:57 - Epoch [4-60/60]\tETA [00h05m37s|00h00m00s]\tTime [0.493|0.032]\tLoss 0.4362 (0.3158)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.738 ( 84.989)\tmAcc  62.729 ( 73.174)\tmIoU  52.259 ( 62.704)\n",
      "Evaluating epoch 5 on val set: 100%|████████████| 54/54 [00:13<00:00,  4.07it/s]\n",
      "INFO - 07/15/25 15:59:05 - 0:02:10 - [val] ------- IoU --------\n",
      "                                     Not burned\t 91.055\n",
      "                                     Burn scar \t 42.849\n",
      "                                     [val]-------------------\n",
      "                                     [val] Mean\t 66.952\n",
      "INFO - 07/15/25 15:59:05 - 0:02:10 - [val] ------- F1-score --------\n",
      "                                     Not burned\t 95.318\n",
      "                                     Burn scar \t 59.992\n",
      "                                     [val]-------------------\n",
      "                                     [val] Mean\t 77.655\n",
      "INFO - 07/15/25 15:59:05 - 0:02:10 - [val] ------- Precision --------\n",
      "                                     Not burned\t 94.968\n",
      "                                     Burn scar \t 61.942\n",
      "                                     [val]-------------------\n",
      "                                     [val] Mean\t 78.455\n",
      "INFO - 07/15/25 15:59:05 - 0:02:10 - [val] ------- Recall --------\n",
      "                                     Not burned\t 95.671\n",
      "                                     Burn scar \t 58.162\n",
      "                                     [val]-------------------\n",
      "                                     [val] Mean\t 76.916\n",
      "INFO - 07/15/25 15:59:05 - 0:02:10 - Mean Accuracy: 91.617 \n",
      "                                     \n",
      "INFO - 07/15/25 15:59:07 - 0:02:12 - Epoch 5 | Training checkpoint saved at checkpoints/20250715_155654_fc10d0_dofa_seg_fcn_hlsburnscars/checkpoint__best.pth\n",
      "INFO - 07/15/25 15:59:07 - 0:02:12 - ============ Starting epoch 5 ... ============\n",
      "INFO - 07/15/25 15:59:07 - 0:02:12 - Epoch [5-1/60]\tETA [00h05m48s|00h00m29s]\tTime [0.493|0.032]\tLoss 0.3170 (0.3134)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.426 ( 85.034)\tmAcc  72.798 ( 73.214)\tmIoU  61.450 ( 62.738)\n",
      "INFO - 07/15/25 15:59:07 - 0:02:12 - Epoch [5-2/60]\tETA [00h05m43s|00h00m28s]\tTime [0.485|0.032]\tLoss 0.5258 (0.3144)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  75.234 ( 84.912)\tmAcc  70.381 ( 73.200)\tmIoU  55.535 ( 62.705)\n",
      "INFO - 07/15/25 15:59:07 - 0:02:12 - Epoch [5-3/60]\tETA [00h05m39s|00h00m27s]\tTime [0.480|0.031]\tLoss 0.3134 (0.3151)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.939 ( 84.868)\tmAcc  73.254 ( 73.179)\tmIoU  55.680 ( 62.576)\n",
      "INFO - 07/15/25 15:59:07 - 0:02:12 - Epoch [5-4/60]\tETA [00h05m35s|00h00m26s]\tTime [0.475|0.031]\tLoss 0.2885 (0.3156)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.357 ( 84.870)\tmAcc  78.601 ( 73.179)\tmIoU  70.608 ( 62.586)\n",
      "INFO - 07/15/25 15:59:08 - 0:02:13 - Epoch [5-5/60]\tETA [00h05m30s|00h00m25s]\tTime [0.468|0.031]\tLoss 0.2229 (0.3141)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.162 ( 84.991)\tmAcc  79.742 ( 73.324)\tmIoU  70.809 ( 62.764)\n",
      "INFO - 07/15/25 15:59:09 - 0:02:13 - Epoch [5-6/60]\tETA [00h05m30s|00h00m25s]\tTime [0.468|0.031]\tLoss 0.1980 (0.3087)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.603 ( 85.087)\tmAcc  85.146 ( 73.460)\tmIoU  79.045 ( 62.943)\n",
      "INFO - 07/15/25 15:59:09 - 0:02:14 - Epoch [5-7/60]\tETA [00h05m30s|00h00m24s]\tTime [0.469|0.031]\tLoss 0.4270 (0.3112)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  79.131 ( 85.073)\tmAcc  70.734 ( 73.490)\tmIoU  58.672 ( 62.963)\n",
      "INFO - 07/15/25 15:59:10 - 0:02:14 - Epoch [5-8/60]\tETA [00h05m30s|00h00m24s]\tTime [0.469|0.031]\tLoss 0.2298 (0.3109)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  90.073 ( 85.176)\tmAcc  81.764 ( 73.704)\tmIoU  73.792 ( 63.201)\n",
      "INFO - 07/15/25 15:59:10 - 0:02:15 - Epoch [5-9/60]\tETA [00h05m30s|00h00m23s]\tTime [0.470|0.031]\tLoss 0.3486 (0.3122)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.052 ( 85.185)\tmAcc  76.198 ( 73.826)\tmIoU  64.271 ( 63.340)\n",
      "INFO - 07/15/25 15:59:11 - 0:02:15 - Epoch [5-10/60]\tETA [00h05m29s|00h00m23s]\tTime [0.470|0.031]\tLoss 0.3387 (0.3099)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.179 ( 85.177)\tmAcc  75.979 ( 73.833)\tmIoU  65.704 ( 63.362)\n",
      "INFO - 07/15/25 15:59:11 - 0:02:16 - Epoch [5-11/60]\tETA [00h05m29s|00h00m23s]\tTime [0.471|0.031]\tLoss 0.3750 (0.3108)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  80.558 ( 85.194)\tmAcc  68.724 ( 73.836)\tmIoU  56.702 ( 63.454)\n",
      "INFO - 07/15/25 15:59:12 - 0:02:16 - Epoch [5-12/60]\tETA [00h05m30s|00h00m22s]\tTime [0.472|0.031]\tLoss 0.2441 (0.3102)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.924 ( 85.217)\tmAcc  78.441 ( 73.866)\tmIoU  69.862 ( 63.500)\n",
      "INFO - 07/15/25 15:59:12 - 0:02:17 - Epoch [5-13/60]\tETA [00h05m30s|00h00m22s]\tTime [0.473|0.031]\tLoss 0.2320 (0.3089)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.845 ( 85.294)\tmAcc  81.873 ( 73.941)\tmIoU  73.295 ( 63.603)\n",
      "INFO - 07/15/25 15:59:13 - 0:02:17 - Epoch [5-14/60]\tETA [00h05m29s|00h00m21s]\tTime [0.473|0.031]\tLoss 0.2862 (0.3078)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.280 ( 85.319)\tmAcc  78.730 ( 73.954)\tmIoU  65.983 ( 63.590)\n",
      "INFO - 07/15/25 15:59:13 - 0:02:18 - Epoch [5-15/60]\tETA [00h05m29s|00h00m21s]\tTime [0.473|0.031]\tLoss 0.2849 (0.3076)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.218 ( 85.351)\tmAcc  77.509 ( 74.003)\tmIoU  65.554 ( 63.624)\n",
      "INFO - 07/15/25 15:59:14 - 0:02:19 - Epoch [5-16/60]\tETA [00h05m27s|00h00m20s]\tTime [0.472|0.031]\tLoss 0.2013 (0.3075)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.330 ( 85.515)\tmAcc  84.780 ( 74.204)\tmIoU  78.124 ( 63.893)\n",
      "INFO - 07/15/25 15:59:14 - 0:02:19 - Epoch [5-17/60]\tETA [00h05m27s|00h00m20s]\tTime [0.472|0.031]\tLoss 0.2732 (0.3022)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.290 ( 85.526)\tmAcc  76.502 ( 74.271)\tmIoU  67.639 ( 63.983)\n",
      "INFO - 07/15/25 15:59:15 - 0:02:20 - Epoch [5-18/60]\tETA [00h05m27s|00h00m19s]\tTime [0.472|0.031]\tLoss 0.2875 (0.3035)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  86.436 ( 85.498)\tmAcc  78.526 ( 74.299)\tmIoU  67.329 ( 64.001)\n",
      "INFO - 07/15/25 15:59:15 - 0:02:20 - Epoch [5-19/60]\tETA [00h05m28s|00h00m19s]\tTime [0.474|0.031]\tLoss 0.3187 (0.3031)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  84.701 ( 85.465)\tmAcc  74.221 ( 74.297)\tmIoU  60.769 ( 63.976)\n",
      "INFO - 07/15/25 15:59:16 - 0:02:21 - Epoch [5-20/60]\tETA [00h05m28s|00h00m18s]\tTime [0.475|0.031]\tLoss 0.2441 (0.3024)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.065 ( 85.594)\tmAcc  83.376 ( 74.451)\tmIoU  72.275 ( 64.153)\n",
      "INFO - 07/15/25 15:59:16 - 0:02:21 - Epoch [5-21/60]\tETA [00h05m28s|00h00m18s]\tTime [0.477|0.031]\tLoss 0.2524 (0.3011)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.465 ( 85.627)\tmAcc  79.096 ( 74.489)\tmIoU  62.111 ( 64.110)\n",
      "INFO - 07/15/25 15:59:17 - 0:02:22 - Epoch [5-22/60]\tETA [00h05m29s|00h00m18s]\tTime [0.478|0.031]\tLoss 0.2515 (0.3015)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  89.545 ( 85.640)\tmAcc  73.301 ( 74.487)\tmIoU  63.162 ( 64.107)\n",
      "INFO - 07/15/25 15:59:17 - 0:02:22 - Epoch [5-23/60]\tETA [00h05m29s|00h00m17s]\tTime [0.480|0.032]\tLoss 0.2217 (0.3004)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.783 ( 85.640)\tmAcc  80.859 ( 74.547)\tmIoU  73.645 ( 64.216)\n",
      "INFO - 07/15/25 15:59:18 - 0:02:23 - Epoch [5-24/60]\tETA [00h05m29s|00h00m17s]\tTime [0.479|0.032]\tLoss 0.2679 (0.2977)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.592 ( 85.668)\tmAcc  75.600 ( 74.578)\tmIoU  67.853 ( 64.273)\n",
      "INFO - 07/15/25 15:59:18 - 0:02:23 - Epoch [5-25/60]\tETA [00h05m28s|00h00m16s]\tTime [0.479|0.032]\tLoss 0.2702 (0.2972)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.292 ( 85.687)\tmAcc  77.625 ( 74.629)\tmIoU  68.004 ( 64.327)\n",
      "INFO - 07/15/25 15:59:19 - 0:02:24 - Epoch [5-26/60]\tETA [00h05m28s|00h00m16s]\tTime [0.479|0.032]\tLoss 0.2333 (0.2947)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  88.814 ( 85.730)\tmAcc  77.742 ( 74.725)\tmIoU  69.883 ( 64.457)\n",
      "INFO - 07/15/25 15:59:19 - 0:02:24 - Epoch [5-27/60]\tETA [00h05m28s|00h00m15s]\tTime [0.480|0.032]\tLoss 0.2284 (0.2936)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.834 ( 85.829)\tmAcc  81.121 ( 74.853)\tmIoU  69.379 ( 64.568)\n",
      "INFO - 07/15/25 15:59:20 - 0:02:25 - Epoch [5-28/60]\tETA [00h05m27s|00h00m15s]\tTime [0.480|0.032]\tLoss 0.2497 (0.2909)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.168 ( 85.899)\tmAcc  77.403 ( 74.937)\tmIoU  66.661 ( 64.635)\n",
      "INFO - 07/15/25 15:59:20 - 0:02:25 - Epoch [5-29/60]\tETA [00h05m26s|00h00m14s]\tTime [0.480|0.032]\tLoss 0.3077 (0.2884)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  85.473 ( 85.889)\tmAcc  69.998 ( 74.935)\tmIoU  58.927 ( 64.598)\n",
      "INFO - 07/15/25 15:59:21 - 0:02:26 - Epoch [5-30/60]\tETA [00h05m26s|00h00m14s]\tTime [0.479|0.032]\tLoss 0.4563 (0.2906)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  74.556 ( 85.789)\tmAcc  64.882 ( 74.884)\tmIoU  51.520 ( 64.500)\n",
      "INFO - 07/15/25 15:59:21 - 0:02:26 - Epoch [5-31/60]\tETA [00h05m25s|00h00m13s]\tTime [0.479|0.032]\tLoss 0.2158 (0.2892)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  92.231 ( 85.866)\tmAcc  77.902 ( 75.036)\tmIoU  65.951 ( 64.626)\n",
      "INFO - 07/15/25 15:59:22 - 0:02:27 - Epoch [5-32/60]\tETA [00h05m24s|00h00m13s]\tTime [0.478|0.032]\tLoss 0.2292 (0.2887)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.236 ( 85.933)\tmAcc  76.097 ( 75.075)\tmIoU  65.166 ( 64.646)\n",
      "INFO - 07/15/25 15:59:22 - 0:02:27 - Epoch [5-33/60]\tETA [00h05m23s|00h00m12s]\tTime [0.478|0.032]\tLoss 0.2372 (0.2882)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.592 ( 86.066)\tmAcc  77.403 ( 75.171)\tmIoU  64.566 ( 64.734)\n",
      "INFO - 07/15/25 15:59:23 - 0:02:28 - Epoch [5-34/60]\tETA [00h05m23s|00h00m12s]\tTime [0.478|0.032]\tLoss 0.3214 (0.2893)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  87.930 ( 86.129)\tmAcc  73.513 ( 75.238)\tmIoU  66.376 ( 64.831)\n",
      "INFO - 07/15/25 15:59:23 - 0:02:28 - Epoch [5-35/60]\tETA [00h05m23s|00h00m11s]\tTime [0.478|0.032]\tLoss 0.1794 (0.2889)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  92.354 ( 86.215)\tmAcc  82.339 ( 75.308)\tmIoU  74.731 ( 64.924)\n",
      "INFO - 07/15/25 15:59:24 - 0:02:29 - Epoch [5-36/60]\tETA [00h05m22s|00h00m11s]\tTime [0.478|0.032]\tLoss 0.3705 (0.2913)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  83.241 ( 86.209)\tmAcc  63.199 ( 75.216)\tmIoU  54.288 ( 64.839)\n",
      "INFO - 07/15/25 15:59:24 - 0:02:29 - Epoch [5-37/60]\tETA [00h05m21s|00h00m10s]\tTime [0.477|0.032]\tLoss 0.3521 (0.2927)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  81.646 ( 86.183)\tmAcc  66.527 ( 75.239)\tmIoU  56.482 ( 64.899)\n",
      "INFO - 07/15/25 15:59:25 - 0:02:30 - Epoch [5-38/60]\tETA [00h05m21s|00h00m10s]\tTime [0.478|0.032]\tLoss 0.4527 (0.2964)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  71.764 ( 86.016)\tmAcc  63.565 ( 75.098)\tmIoU  48.164 ( 64.687)\n",
      "INFO - 07/15/25 15:59:25 - 0:02:30 - Epoch [5-39/60]\tETA [00h05m22s|00h00m10s]\tTime [0.480|0.032]\tLoss 0.2082 (0.2925)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.595 ( 86.045)\tmAcc  77.061 ( 75.100)\tmIoU  71.125 ( 64.750)\n",
      "INFO - 07/15/25 15:59:26 - 0:02:31 - Epoch [5-40/60]\tETA [00h05m21s|00h00m09s]\tTime [0.479|0.033]\tLoss 0.2196 (0.2928)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  91.676 ( 86.076)\tmAcc  81.563 ( 75.106)\tmIoU  76.239 ( 64.811)\n",
      "INFO - 07/15/25 15:59:26 - 0:02:31 - Epoch [5-41/60]\tETA [00h05m20s|00h00m09s]\tTime [0.479|0.032]\tLoss 0.3831 (0.2957)\tlr 1.000e-04\n",
      "                                      Training metrics: Acc  82.885 ( 86.114)\tmAcc  69.679 ( 75.146)\tmIoU  59.998 ( 64.871)\n"
     ]
    }
   ],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_fcn\\\n",
    "    preprocessing=seg_default\\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84361e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_linear\\\n",
    "    preprocessing=seg_default\\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceac53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_segformer\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=dofa\\\n",
    "    decoder=seg_upernet\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/DOFA_ViT_base_e100.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78614820",
   "metadata": {},
   "source": [
    "#### Prithvi 1.0\n",
    "[Link to paper](https://arxiv.org/pdf/2310.18660)\n",
    "\n",
    "One of the earliest foundation models and the simplest of the three models tested in this demonstration. Developed by the NASA IMPACT, Prithvi 1.0 is a vanilla masked autoencoder specifically trained exclusively on HLS data. This limited scope presents a unique research opportunity for studying a vision transformer with relatively few modifications to its structure. The expectation then is that for this specific test, Prithvi would have the best performance metrics given that its pretraining dataset more closely aligns with the downstream task. TBD!\n",
    "\n",
    "\n",
    "<img src=\"./assets/prithvi.png\"/>\n",
    "\n",
    "Figure 7: Masked autoencoder diagram for Prithvi's pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d610e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HYDRA_FULL_ERROR=1 torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi \\\n",
    "    decoder=seg_fcn\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi \\\n",
    "    decoder=seg_linear\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b85067",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi\\\n",
    "    decoder=seg_segformer\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c2b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=prithvi \\\n",
    "    decoder=seg_upernet\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/Prithvi_EO_V1_100M.pt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177798b2",
   "metadata": {},
   "source": [
    "#### CHROMA\n",
    "[Link to paper](https://arxiv.org/pdf/2311.00566)\n",
    "\n",
    "CHROMA is a another transformer-based model with a slightly different approach combining both masked image modeling (MIM) and contrastive learning. Rather than contrasting positive and negative samples, the CHROMA framework contrasts optical and radar data using separate encoders then combines those embeddings using cross attention with a unifying transformer encoder. The output of this terminal encoder is then randomly masked and trained in a typical encoder-decoder MIM style. The optical encoder requires all 12 spectral bands from Sentinel 2 while the radar encoder requires the 2 polarization bands from Sentinel 1. In our downstream task, we are limited to 6 harmonized spectral bands. How can we expect this to impact our results?\n",
    "\n",
    "<img src=\"./assets/chroma.png\"/>\n",
    "\n",
    "Figure 8: (Left) Pretraining framework for CHROMA. (Right) Encoding workflow for leveraging internal representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a83af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_fcn\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/CROMA_large.pt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_segformer\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/CROMA_large.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a792a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_linear\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/CROMA_large.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f662a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun pangaea-bench/pangaea/run.py \\\n",
    "    --config-name=train \\\n",
    "    work_dir=checkpoints \\\n",
    "    dataset=hlsburnscars \\\n",
    "    encoder=croma_optical\\\n",
    "    decoder=seg_upernet\\\n",
    "    preprocessing=seg_default \\\n",
    "    criterion=cross_entropy \\\n",
    "    task=segmentation \\\n",
    "    use_wandb=true \\\n",
    "    task.trainer.n_epochs=16 \\\n",
    "    task.trainer.log_interval=1 \\\n",
    "    task.trainer.ckpt_interval=16 \\\n",
    "    encoder.encoder_weights=pretrained_models/CROMA_large.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b0e07",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af6bf0b",
   "metadata": {},
   "source": [
    "Performance metrics are not always straightforward to interpret and at the same time, comparison of metrics introduces another layer of complexity to benchmarking.\n",
    "\n",
    "#### Per Class Metrics\n",
    "\n",
    "#### Decoder Choice\n",
    "\n",
    "#### Run Times\n",
    "\n",
    "#### Compute Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f44dd5",
   "metadata": {},
   "source": [
    "### Final Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7da3ff",
   "metadata": {},
   "source": [
    "#### A Well Defined Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620074fc",
   "metadata": {},
   "source": [
    "#### Multimodality / Multisensor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d23963",
   "metadata": {},
   "source": [
    "#### Scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangaea-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
